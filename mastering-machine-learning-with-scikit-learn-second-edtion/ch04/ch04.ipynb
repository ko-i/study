{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2020-10-18 created by Akson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# code4.1\n",
    "# one-hot编码测试\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "onehot_encoder = DictVectorizer()\n",
    "X = [{'city': 'New York'}, {'city': 'San Francisco'}, {'city': 'Chapel Hill'}]\n",
    "\n",
    "print(onehot_encoder.fit_transform(X).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.70710678 -1.38873015  0.52489066  0.59299945 -1.35873244]\n",
      " [ 0.         -0.70710678  0.46291005  0.87481777  0.81537425  1.01904933]\n",
      " [ 0.          1.41421356  0.9258201  -1.39970842 -1.4083737   0.33968311]]\n"
     ]
    }
   ],
   "source": [
    "# code4.2\n",
    "# 特征标准化\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[0., 0., 5., 13., 9., 1.],\n",
    "              [0., 0., 13., 15., 10., 15.],\n",
    "              [0., 3., 15., 2., 0., 11.]])\n",
    "\n",
    "print(preprocessing.scale(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code4.3\n",
    "\n",
    "corpus = ['UNC played Duke in basketball', 'Duke lost the basketball game']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1 0 1 0 1]\n",
      " [1 1 1 0 1 0 1 0]]\n",
      "{'unc': 7, 'played': 5, 'duke': 1, 'in': 3, 'basketball': 0, 'lost': 4, 'the': 6, 'game': 2}\n"
     ]
    }
   ],
   "source": [
    "# code4.4\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1 0 1 0 0 1]\n",
      " [0 1 1 1 0 1 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 1 0 0]]\n",
      "{'unc': 9, 'played': 6, 'duke': 2, 'in': 4, 'basketball': 1, 'lost': 5, 'the': 8, 'game': 3, 'ate': 0, 'sandwich': 7}\n"
     ]
    }
   ],
   "source": [
    "# code4.5\n",
    "\n",
    "corpus.append('I ate a sandwich')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between 1st and 2nd documents: [[2.44948974]]\n",
      "Distance between 1st and 3rd documents: [[2.64575131]]\n",
      "Distance between 2nd and 3rd documents: [[2.64575131]]\n"
     ]
    }
   ],
   "source": [
    "# code4.6\n",
    "# 使用欧几里得范数对文档进行特征向量比较\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "X = vectorizer.fit_transform(corpus).todense()\n",
    "\n",
    "print('Distance between 1st and 2nd documents:', euclidean_distances(X[0], X[1]))\n",
    "print('Distance between 1st and 3rd documents:', euclidean_distances(X[0], X[2]))\n",
    "print('Distance between 2nd and 3rd documents:', euclidean_distances(X[1], X[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 1 0 1]\n",
      " [0 1 1 1 1 0 0 0]\n",
      " [1 0 0 0 0 0 1 0]]\n",
      "{'unc': 7, 'played': 5, 'duke': 2, 'basketball': 1, 'lost': 4, 'game': 3, 'ate': 0, 'sandwich': 6}\n"
     ]
    }
   ],
   "source": [
    "# code4.7\n",
    "# 停用词测试\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = 'english')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1]\n",
      " [0 1 1 0]]\n",
      "{'ate': 0, 'sandwiches': 3, 'sandwich': 2, 'eaten': 1}\n"
     ]
    }
   ],
   "source": [
    "# code4.8\n",
    "\n",
    "corpus = ['He ate the sandwiches', 'Every sandwich was eaten by him']\n",
    "vectorizer = CountVectorizer(binary = True, stop_words = 'english')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code4.9\n",
    "corpus = ['I am gathering ingredients for the sandwich.', 'There were many wizards at the gathering.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather\n",
      "gathering\n"
     ]
    }
   ],
   "source": [
    "# code4.10\n",
    "# 词形还原\n",
    "\n",
    "# 第一次使用时运行以下两行可以自动安装这个wordnet包\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('gathering', 'v'))\n",
    "print(lemmatizer.lemmatize('gathering', 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather\n"
     ]
    }
   ],
   "source": [
    "# code4.11\n",
    "# 词干提取\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('gathering'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed: [['He', 'ate', 'the', 'sandwich'], ['everi', 'sandwich', 'wa', 'eaten', 'by', 'him']]\n",
      "Lemmatized: [['He', 'eat', 'the', 'sandwich'], ['Every', 'sandwich', 'be', 'eat', 'by', 'him']]\n"
     ]
    }
   ],
   "source": [
    "# code4.12\n",
    "# 对语料库进行词形还原\n",
    "\n",
    "# 第一次运行还请输入以下三条语句安装依赖包\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "wordnet_tags = ['n', 'v']\n",
    "corpus = ['He ate the sandwiches', 'Every sandwich was eaten by him']\n",
    "stemmer = PorterStemmer()\n",
    "print('Stemmed:', [[stemmer.stem(token) for token in word_tokenize(document)] for document in corpus])\n",
    "\n",
    "# 如果是名词或动词就考虑转换一下，不是就不转了\n",
    "def lemmatize(token, tag):\n",
    "    if tag[0].lower() in ['n', 'v']:\n",
    "        # print(lemmatizer.lemmatize(token, tag[0].lower()))\n",
    "        return lemmatizer.lemmatize(token, tag[0].lower())\n",
    "    return token\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 获得一段语料库中的每个词的词性\n",
    "tagged_corpus = [pos_tag(word_tokenize(document)) for document in corpus]\n",
    "# print(tagged_corpus)\n",
    "\n",
    "print('Lemmatized:', [[lemmatize(token, tag) for token, tag in document] for document in tagged_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 3 1 1]\n",
      "Token indices {'dog': 1, 'ate': 0, 'sandwich': 2, 'wizard': 4, 'transfigured': 3}\n",
      "The token \"dog\" appears 1 times\n",
      "The token \"ate\" appears 2 times\n",
      "The token \"sandwich\" appears 3 times\n",
      "The token \"wizard\" appears 1 times\n",
      "The token \"transfigured\" appears 1 times\n"
     ]
    }
   ],
   "source": [
    "# code4.13\n",
    "# 基于词频数的表示\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['The dog ate a sandwich, the wizard transfigured a sandwich, and I ate a sandwich']\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = 'english')  # 这里参数中去掉了binary = True\n",
    "frequencies = np.array(vectorizer.fit_transform(corpus).todense())[0]\n",
    "print(frequencies)\n",
    "print('Token indices %s' % vectorizer.vocabulary_)\n",
    "\n",
    "for token, index in vectorizer.vocabulary_.items():\n",
    "    print('The token \"%s\" appears %s times' % (token, frequencies[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.75458397 0.37729199 0.53689271 0.         0.        ]\n",
      " [0.         0.         0.44943642 0.6316672  0.6316672 ]]\n"
     ]
    }
   ],
   "source": [
    "# code4.14\n",
    "# 词频-逆文档频率表示\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = ['The dog ate a sandwich and I ate a sandwich',\n",
    "          'The wizard transfigured a sandwich']\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "# print('Token indices %s' % vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# code4.15\n",
    "# 哈希技巧\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "corpus = ['the', 'ate', 'bacon', 'cat']\n",
    "vectorizer = HashingVectorizer(n_features = 6)\n",
    "print(vectorizer.transform(corpus).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code4.16\n",
    "\n",
    "# 这里需要在本python环境终端中输入pip install gensim来安装这个依赖包(conda install不好使)\n",
    "import gensim\n",
    "\n",
    "# 需要自行下载GoogleNews-vectors-negative300.bin.gz并解压成GoogleNews-vectors-negative300.bin再放入此目录中\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary = True)\n",
    "\n",
    "# 因为引入时间较长，为了方便测试我把它分成了多块代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: 300\n",
      "[ 0.0123291   0.20410156 -0.28515625  0.21679688  0.11816406  0.08300781\n",
      "  0.04980469 -0.00952148  0.22070312 -0.12597656  0.08056641 -0.5859375\n",
      " -0.00445557 -0.296875   -0.01312256 -0.08349609  0.05053711  0.15136719\n",
      " -0.44921875 -0.0135498   0.21484375 -0.14746094  0.22460938 -0.125\n",
      " -0.09716797  0.24902344 -0.2890625   0.36523438  0.41210938 -0.0859375\n",
      " -0.07861328 -0.19726562 -0.09082031 -0.14160156 -0.10253906  0.13085938\n",
      " -0.00346375  0.07226562  0.04418945  0.34570312  0.07470703 -0.11230469\n",
      "  0.06738281  0.11230469  0.01977539 -0.12353516  0.20996094 -0.07226562\n",
      " -0.02783203  0.05541992 -0.33398438  0.08544922  0.34375     0.13964844\n",
      "  0.04931641 -0.13476562  0.16308594 -0.37304688  0.39648438  0.10693359\n",
      "  0.22167969  0.21289062 -0.08984375  0.20703125  0.08935547 -0.08251953\n",
      "  0.05957031  0.10205078 -0.19238281 -0.09082031  0.4921875   0.03955078\n",
      " -0.07080078 -0.0019989  -0.23046875  0.25585938  0.08984375 -0.10644531\n",
      "  0.00105286 -0.05883789  0.05102539 -0.0291748   0.19335938 -0.14160156\n",
      " -0.33398438  0.08154297 -0.27539062  0.10058594 -0.10449219 -0.12353516\n",
      " -0.140625    0.03491211 -0.11767578 -0.1796875  -0.21484375 -0.23828125\n",
      "  0.08447266 -0.07519531 -0.25976562 -0.21289062 -0.22363281 -0.09716797\n",
      "  0.11572266  0.15429688  0.07373047 -0.27539062  0.14257812 -0.0201416\n",
      "  0.10009766 -0.19042969 -0.09375     0.14160156  0.17089844  0.3125\n",
      " -0.16699219 -0.08691406 -0.05004883 -0.24902344 -0.20800781 -0.09423828\n",
      " -0.12255859 -0.09472656 -0.390625   -0.06640625 -0.31640625  0.10986328\n",
      " -0.00156403  0.04345703  0.15625    -0.18945312 -0.03491211  0.03393555\n",
      " -0.14453125  0.01611328 -0.14160156 -0.02392578  0.01501465  0.07568359\n",
      "  0.10742188  0.12695312  0.10693359 -0.01184082 -0.24023438  0.0291748\n",
      "  0.16210938  0.19921875 -0.28125     0.16699219 -0.11621094 -0.25585938\n",
      "  0.38671875 -0.06640625 -0.4609375  -0.06176758 -0.14453125 -0.11621094\n",
      "  0.05688477  0.03588867 -0.10693359  0.18847656 -0.16699219 -0.01794434\n",
      "  0.10986328 -0.12353516 -0.16308594 -0.14453125  0.12890625  0.11523438\n",
      "  0.13671875  0.05688477 -0.08105469 -0.06152344 -0.06689453  0.27929688\n",
      " -0.19628906  0.07226562  0.12304688 -0.20996094 -0.22070312  0.21386719\n",
      " -0.1484375  -0.05932617  0.05224609  0.06445312 -0.02636719  0.13183594\n",
      "  0.19433594  0.27148438  0.18652344  0.140625    0.06542969 -0.14453125\n",
      "  0.05029297  0.08837891  0.12255859  0.26757812  0.0534668  -0.32226562\n",
      " -0.20703125  0.18164062  0.04418945 -0.22167969 -0.13769531 -0.04174805\n",
      " -0.00286865  0.04077148  0.07275391 -0.08300781  0.08398438 -0.3359375\n",
      " -0.40039062  0.01757812 -0.18652344 -0.0480957  -0.19140625  0.10107422\n",
      "  0.09277344 -0.30664062 -0.19921875 -0.0168457   0.12207031  0.14648438\n",
      " -0.12890625 -0.23535156 -0.05371094 -0.06640625  0.06884766 -0.03637695\n",
      "  0.2109375  -0.06005859  0.19335938  0.05151367 -0.05322266  0.02893066\n",
      " -0.27539062  0.08447266  0.328125    0.01818848  0.01495361  0.04711914\n",
      "  0.37695312 -0.21875    -0.03393555  0.01116943  0.36914062  0.02160645\n",
      "  0.03466797  0.07275391  0.16015625 -0.16503906 -0.296875    0.15039062\n",
      " -0.29101562  0.13964844  0.00448608  0.171875   -0.21972656  0.09326172\n",
      " -0.19042969  0.01599121 -0.09228516  0.15722656 -0.14160156 -0.0534668\n",
      "  0.03613281  0.23632812 -0.15136719 -0.00689697 -0.27148438 -0.07128906\n",
      " -0.16503906  0.18457031 -0.08398438  0.18554688  0.11669922  0.02758789\n",
      " -0.04760742  0.17871094  0.06542969 -0.03540039  0.22949219  0.02697754\n",
      " -0.09765625  0.26953125  0.08349609 -0.13085938 -0.10107422 -0.00738525\n",
      "  0.07128906  0.14941406 -0.20605469  0.18066406 -0.15820312  0.05932617\n",
      "  0.28710938 -0.04663086  0.15136719  0.4921875  -0.27539062  0.05615234]\n"
     ]
    }
   ],
   "source": [
    "# 打印“cat”的词向量试试\n",
    "embedding = model.word_vec('cat')\n",
    "print('Dimensions: %s' % embedding.shape)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76094574\n",
      "0.17211203\n"
     ]
    }
   ],
   "source": [
    "# 测试一些单词的相似性\n",
    "print(model.similarity('cat', 'dog'))\n",
    "print(model.similarity('cat', 'sandwich'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dog', 0.7762665748596191)]\n"
     ]
    }
   ],
   "source": [
    "# 看看‘puppy’对于‘cat’相当于‘kitten’对于什么\n",
    "\n",
    "print(model.most_similar(positive = ['puppy', 'cat'], negative = ['kitten'], topn = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('saddles', 0.5282258987426758)\n",
      "('horseman', 0.5179382562637329)\n",
      "('jockey', 0.48861294984817505)\n"
     ]
    }
   ],
   "source": [
    "# 再试试别的\n",
    "\n",
    "for i in model.most_similar(positive = ['saddle', 'painter'], negative = ['palette'], topn = 3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit: 0\n",
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n",
      "Feature vector: \n",
      " [[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      "  15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "   0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "   0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# code4.17\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "print('Digit: %s' % digits.target[0])\n",
    "print(digits.images[0])\n",
    "print('Feature vector: \\n %s' % digits.images[0].reshape(-1, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code4.18\n",
    "# 这个caffe有毒，深度学习先放一放"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
