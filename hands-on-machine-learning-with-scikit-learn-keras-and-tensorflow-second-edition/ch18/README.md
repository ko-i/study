# 第18章 强化学习

---

### 练习题
---

###### 1. 你如何定义强化学习？它与常规的有监督学习或无监督学习有何不同？
> 强化学习是机器学习的一个领域，意在创建能够在环境中采取行动的智能体，从而使得奖励随着时间的推移而最大化。RL与常规有监督学习和无监督学习之间的差异有：
> * 再有监督和无监督学习中，目标通常是在数据中找到模式并使用他们来进行预测。在强化学习中，目标是找到一个好的策略。
> * 与有监督学习不同，强化学习没有明确为智能体给出“正确”的答案。它必须通过反复试验错误来学习。
> * 与无监督学习不同，有一种通过奖励的有监督形式。我们不告诉智能体如何执行任务，但是会告诉智能体其成功或失败。
> * 强化学习代理需要在探索环境，寻找获得奖励的新方法以及利用已经知道的奖励来源之间找到适当的平衡。相反，有监督学习和无监督学习系统通常不需要担心探索。他们只是根据给定的训练数据。
> * 再有监督学习和无监督学习中，训练实例通常是独立的（实际上，他们通常是随机混洗的）。在强化学习中，连续观察通常不是独立的。智能体在连续前进之前可能会在环境的同一区域内停留一段时间，因此连续的观察结果将非常相关。在某些情况下，重播存储用于确保训练算法能得到相当独立的观察结果。
---
###### 2. 你能想到本章节中那个未提到的RL的三种可能的应用嘛？对于每一个来说，环境是什么？什么是智能体？有那些可能的动作？有什么奖励？
> * 音乐个性化：环境是用户的个性化网络广播。智能体是决定该用户接下来要播放什么歌曲的软件。
> * 市场营销：环境是你公司的营销部门。智能体是一种软件，他根据给定的个人资料和历史购买记录来定义应将邮件送给哪个客户。
> * 产品交付：让智能体控制一批货车，确定他们应该在仓库取什么货，应该去的地方，应该卸什么货。
---
###### 3. 折扣因子是多少？如果你修改折扣因子，最优策略会改变吗？
> 估计动作的值时，强化学习算法通常会汇总该动作带来的所有奖励，将更多的权重分配给及时奖励，将较少的权重分配给以后的奖励。为了对此建模，通常在每个时间步长应用折扣因子。例如，在折扣因子为0.9的情况下，当你估算操作的值时，在两个时间不长之后收到的100的奖励仅记为81。你可以将折扣因子十位权衡相对于当前的未来价值的度量：如果他非常接近于1，则未来的值几乎与现在的值相同；如果接近于0，则仅是立即获得奖励很重要。当然，这会对最优策略产生巨大影响：如果你看中未来，你可能愿意为最终回报的前景承担更多当即的痛苦；如果你不看重未来，则只会抓住你可以找到的任何可以立即获得的回报，从不对未来进行投资。
---
###### 4. 你如何衡量强化学习智能体的性能？
> 要衡量强化学习智能体的性能，你可以简单的汇总他所获得的奖励。在模拟环境中，你可以运行许多个回合，查看其平均获得的总奖励。
---
###### 5. 什么是贡献分配问题？什么时候发生？如何缓解呢？
> 信用分配问题是这样的事实：当强化学习智能体收到奖励时，他没有直接的方法来指导先前的哪些行为促成了该奖励。者通常在动作和所得奖励之间存在较大延迟时发生。解决它的一种方法是在可能的情况下为智能体提供短期奖励。者通常需要有关任务的先验知识。
---
###### 6. 使用重播缓冲区有什么意义？
> 智能体通常可以在其环境的同一区域中停留一段时间，因此在这段时间内，其所有的经历都非常相似。这可能会在学习算法中引入一些偏差。他可能会对针对此环境区域调整其策略，但是一旦移出该区域，它的性能会不好。要解决此问题，你可以使用重放存储。智能体不用最近的学习经历，而是将基于过去的经历的缓冲来学习。
---
###### 7. 什么是异策略RL算法？
> 异策略RL算法学习最佳策略的值（如果智能体采取最佳行动，则每个状态可以预期的折扣奖励总和），而智能体遵循不同的策略。Q学习是这种算法的一个很好地示例。相反，同策略的算法学习智能体实际执行的策略的值，包括探索和利用。
---