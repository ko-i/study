{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch18.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM4c9iWEdxiW"
      },
      "source": [
        "# 2020-11-02 created by Akson"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9zBhKGSdysw"
      },
      "source": [
        "# Code18.1\n",
        "# 安装显示用的依赖包（colab中运行）\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    !apt update && apt install -y libpq-dev libsdl2-dev swig xorg-dev xvfb \n",
        "    # !pip install -q -U tf-agents-nightly pyvirtualdisplay gym[atari]\n",
        "    !pip install -q -U tf-agents pyvirtualdisplay gym[atari]\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKgk2ZfZmuwB"
      },
      "source": [
        "# Code18.2\n",
        "# import\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "import matplotlib.animation as animation\n",
        "mpl.rc('animation', html='jshtml')\n",
        "\n",
        "import gym\n",
        "\n",
        "try:\n",
        "    import pyvirtualdisplay\n",
        "    display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n",
        "except ImportError:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huWa24w0fOiU"
      },
      "source": [
        "# Code18.3\n",
        "# gym test\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "obs = env.reset()\n",
        "print(obs)\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVvi9j48qg0P"
      },
      "source": [
        "# Code18.4\n",
        "# plot image func\n",
        "\n",
        "def plot_environment(env, figsize = (5, 4)):\n",
        "    plt.figure(figsize = figsize)\n",
        "    img = env.render(mode = 'rgb_array')\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddHs2-5jq5qb"
      },
      "source": [
        "# Code18.5\n",
        "# plot env img\n",
        "\n",
        "plot_environment(env)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fBawVIBqU7j"
      },
      "source": [
        "# Code18.6\n",
        "# other result\n",
        "\n",
        "print(env.action_space)\n",
        "action = 1\n",
        "obs, reward, done, info = env.step(action)\n",
        "print(obs)\n",
        "print(reward)\n",
        "print(done)\n",
        "print(info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVIkhhbKrlIC"
      },
      "source": [
        "# Code18.7\n",
        "# basic_policy\n",
        "\n",
        "def basic_policy(obs):\n",
        "    angle = obs[2]\n",
        "    return 0 if angle < 0 else 1\n",
        "\n",
        "totals = []\n",
        "for episode in range(500):\n",
        "    episode_rewards = 0\n",
        "    obs = env.reset()\n",
        "    for step in range(200):\n",
        "        action = basic_policy(obs)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        episode_rewards += reward\n",
        "        if done:\n",
        "            break\n",
        "    totals.append(episode_rewards)\n",
        "\n",
        "print(np.mean(totals), np.std(totals), np.min(totals), np.max(totals))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj9SHiElu6AN"
      },
      "source": [
        "# Code18.8\n",
        "# plot animation func\n",
        "\n",
        "def update_scene(num, frames, patch):\n",
        "    patch.set_data(frames[num])\n",
        "    return patch\n",
        "\n",
        "def plot_animation(frams, repeat = False, interval = 40):\n",
        "    fig = plt.figure()\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    anim = animation.FuncAnimation(fig, update_scene, fargs = (frames, patch), frames = len(frames), repeat = repeat, interval = interval)\n",
        "    plt.close()\n",
        "    return anim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRViNqN6wMzk"
      },
      "source": [
        "# Code18.9\n",
        "# draw it\n",
        "\n",
        "frames = []\n",
        "obs = env.reset()\n",
        "for step in range(200):\n",
        "    img = env.render(mode = 'rgb_array')\n",
        "    frames.append(img)\n",
        "    action = basic_policy(obs)\n",
        "\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "plot_animation(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrD78YDft-0I"
      },
      "source": [
        "# Code18.10\n",
        "# create NN model\n",
        "\n",
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "n_inputs = 4\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(5, activation = 'elu', input_shape = [n_inputs]),\n",
        "    keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1UDZV284YL_"
      },
      "source": [
        "# Code18.11\n",
        "# train func\n",
        "\n",
        "def render_policy_net(model, n_max_steps = 200, seed = 42):\n",
        "    frames = []\n",
        "    env = gym.make('CartPole-v1')\n",
        "    env.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    obs = env.reset()\n",
        "\n",
        "    for step in range(n_max_steps):\n",
        "        frames.append(env.render(mode = 'rgb_array'))\n",
        "        left_proba = model.predict(obs.reshape(1, -1))\n",
        "        action = int(np.random.rand() > left_proba)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    env.close()\n",
        "    return frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ0yTziT5SwM"
      },
      "source": [
        "# Code18.12\n",
        "# train it\n",
        "\n",
        "frames = render_policy_net(model)\n",
        "plot_animation(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILdwe5dF5fac"
      },
      "source": [
        "# Code18.13\n",
        "# 一些准备函数\n",
        "\n",
        "# 前进一步\n",
        "def play_one_step(env, obs, model, loss_fn):\n",
        "    with tf.GradientTape() as tape:\n",
        "        left_proba = model(obs[np.newaxis])\n",
        "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
        "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
        "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    obs, reward, done, info = env.step(int(action[0, 0].numpy()))\n",
        "\n",
        "    return obs, reward, done, grads\n",
        "\n",
        "# 玩多步\n",
        "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
        "    all_rewards = []\n",
        "    all_grads = []\n",
        "    for episode in range(n_episodes):\n",
        "        current_rewards = []\n",
        "        current_grads = []\n",
        "        obs = env.reset()\n",
        "        for step in range(n_max_steps):\n",
        "            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n",
        "            current_rewards.append(reward)\n",
        "            current_grads.append(grads)\n",
        "            if done:\n",
        "                break\n",
        "        all_rewards.append(current_rewards)\n",
        "        all_grads.append(current_grads)\n",
        "    return all_rewards, all_grads\n",
        "\n",
        "# 计算这一批每一步的折扣奖励\n",
        "def discount_rewards(rewards, discount_rate):\n",
        "    discounted = np.array(rewards)\n",
        "    for step in range(len(rewards) -2, -1, -1):\n",
        "        discounted[step] += discounted[step + 1] * discount_rate\n",
        "    return discounted\n",
        "\n",
        "# 将这一批奖励折扣化和标准化\n",
        "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
        "    all_discounted_rewards = [discount_rewards(rewards, discount_rate) for rewards in all_rewards]\n",
        "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
        "    reward_mean = flat_rewards.mean()\n",
        "    reward_std = flat_rewards.std()\n",
        "\n",
        "    return [(discounted_rewards - reward_mean) / reward_std for discounted_rewards in all_discounted_rewards]\n",
        "\n",
        "# test\n",
        "print(discount_rewards([10, 0, -50], discount_rate = 0.8))\n",
        "print(discount_and_normalize_rewards([[10, 0, -50], [10, 20]], discount_rate = 0.8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aDfhImRD3iL"
      },
      "source": [
        "# Code18.14\n",
        "# pepare some factor\n",
        "\n",
        "n_iterations = 150\n",
        "n_episodes_per_update = 10\n",
        "n_max_steps = 200\n",
        "discount_rate = 0.95\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr = 0.01)\n",
        "loss_fn = keras.losses.binary_crossentropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mxfFcFZFy-o"
      },
      "source": [
        "# Code18.15\n",
        "# create model\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(5, activation = 'elu', input_shape = [4]),\n",
        "    keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mKmE5y1GKl6"
      },
      "source": [
        "# Code18.16\n",
        "# train it\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "env.seed(42)\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    all_rewards, all_grads = play_multiple_episodes(env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
        "    total_rewards = sum(map(sum, all_rewards))\n",
        "    print('\\rIteration: {}, mean rewards: {:.1f}'.format(iteration, total_rewards / n_episodes_per_update), end = '')\n",
        "    all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_rate)\n",
        "    all_mean_grads = []\n",
        "    for var_index in range(len(model.trainable_variables)):\n",
        "        mean_grads = tf.reduce_mean([final_reward * all_grads[episode_index][step][var_index] for episode_index, final_rewards in enumerate(all_final_rewards) for step, final_reward in enumerate(final_rewards)], axis = 0)\n",
        "        all_mean_grads.append(mean_grads)\n",
        "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
        "\n",
        "env.close()\n",
        "frames = render_policy_net(model)\n",
        "plot_animation(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COEbREJmPyj9"
      },
      "source": [
        "# Code18.17\n",
        "# print markov chain\n",
        "\n",
        "transition_probabilities = [\n",
        "    [0.7, 0.2, 0.0, 0.1],\n",
        "    [0.0, 0.0, 0.9, 0.1],\n",
        "    [0.0, 1.0, 0.0, 0.0],\n",
        "    [0.0, 0.0, 0.0, 1.0]\n",
        "]\n",
        "\n",
        "n_max_steps = 50\n",
        "\n",
        "def print_sequence():\n",
        "    current_state = 0\n",
        "    print('States:', end = ' ')\n",
        "    for step in range(n_max_steps):\n",
        "        print(current_state, end = ' ')\n",
        "        if current_state == 3:\n",
        "            break\n",
        "        current_state = np.random.choice(range(4), p = transition_probabilities[current_state])\n",
        "    else: \n",
        "        print('...', end = '')\n",
        "    print()\n",
        "\n",
        "for _ in range(10):\n",
        "    print_sequence()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er1UyqSDRVSB"
      },
      "source": [
        "# Code18.18\n",
        "# some dataset\n",
        "\n",
        "transition_probabilities = [\n",
        "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
        "    [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
        "    [None, [0.8, 0.1, 0.1], None]\n",
        "]\n",
        "\n",
        "rewards = [\n",
        "    [[10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
        "    [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
        "    [[0, 0, 0], [40, 0, 0], [0, 0, 0]]\n",
        "]\n",
        "\n",
        "possible_actions = [[0, 1, 2], [0, 2], [1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gBlRq3LUdg_"
      },
      "source": [
        "# Code18.19\n",
        "# Q-iteration\n",
        "\n",
        "def cal_Q_value(gamma, transition_probabilities, rewards):\n",
        "    Q_values = np.full((3, 3), -np.inf)\n",
        "    for state, actions in enumerate(possible_actions):\n",
        "        Q_values[state, actions] = 0.0\n",
        "    \n",
        "    for iteration in range(50):\n",
        "        Q_prev = Q_values.copy()\n",
        "        for s in range(3):\n",
        "            for a in possible_actions[s]:\n",
        "                Q_values[s, a] = np.sum([transition_probabilities[s][a][sp] * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp])) for sp in range(3)])\n",
        "    return Q_values\n",
        "\n",
        "Q_values = cal_Q_value(0.95, transition_probabilities, rewards)\n",
        "print(Q_values)\n",
        "print(np.argmax(Q_values, axis = 1))\n",
        "\n",
        "Q_values = cal_Q_value(0.90, transition_probabilities, rewards)\n",
        "print(Q_values)\n",
        "print(np.argmax(Q_values, axis = 1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkwR64_OYndH"
      },
      "source": [
        "# Code18.20\n",
        "# Q-learning\n",
        "\n",
        "def step(state, action):\n",
        "    probas = transition_probabilities[state][action]\n",
        "    next_state = np.random.choice([0, 1, 2], p = probas)\n",
        "    reward = rewards[state][action][next_state]\n",
        "    return next_state, reward\n",
        "\n",
        "def exploration_policy(state):\n",
        "    return np.random.choice(possible_actions[state])\n",
        "\n",
        "Q_values = np.full((3, 3), -np.inf)\n",
        "for state, actions in enumerate(possible_actions):\n",
        "    Q_values[state][actions] = 0\n",
        "\n",
        "alpha0 = 0.05\n",
        "decay = 0.005\n",
        "gamma = 0.90\n",
        "state = 0\n",
        "\n",
        "for iteration in range(10000):\n",
        "    action = exploration_policy(state)\n",
        "    next_state, reward = step(state, action)\n",
        "    next_value = np.max(Q_values[next_state])\n",
        "    alpha = alpha0 / (1 + iteration * decay)\n",
        "    Q_values[state, action] *= 1 - alpha\n",
        "    Q_values[state, action] += alpha * (reward + gamma * next_value)\n",
        "    state = next_state\n",
        "\n",
        "print(Q_values)\n",
        "print(np.argmax(Q_values, axis = 1)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lew74Ganc4nC"
      },
      "source": [
        "# Code18.21\n",
        "# DQN\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "# create model\n",
        "input_shape = [4]\n",
        "n_outputs = 2\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(32, activation = 'elu', input_shape = input_shape),\n",
        "    keras.layers.Dense(32, activation = 'elu'),\n",
        "    keras.layers.Dense(n_outputs)\n",
        "])\n",
        "\n",
        "# 确定行动策略\n",
        "def epsilon_greedy_policy(state, epsilon = 0):\n",
        "    # 采用随机行动\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(2)\n",
        "    # 采取当前最大Q值的策略\n",
        "    else:\n",
        "        Q_values = model.predict(state[np.newaxis])\n",
        "        return np.argmax(Q_values[0])\n",
        "\n",
        "# 定义重放缓冲区\n",
        "from collections import deque\n",
        "replay_memory = deque(maxlen = 2000)\n",
        "\n",
        "# 从重放缓冲区中随机抽取一批经验\n",
        "def sample_experiences(batch_size):\n",
        "    indices = np.random.randint(len(replay_memory), size = batch_size)\n",
        "    batch = [replay_memory[index] for index in indices]\n",
        "    states, actions, rewards, next_states, dones = [np.array([experience[field_index] for experience in batch]) for field_index in range(5)]\n",
        "    return states, actions, rewards, next_states, dones\n",
        "\n",
        "# 行动一步\n",
        "def play_one_step(env, state, epsilon):\n",
        "    action = epsilon_greedy_policy(state, epsilon)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    replay_memory.append((state, action, reward, next_state, done))\n",
        "    return next_state, reward, done, info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK7WCrlpjt3n"
      },
      "source": [
        "# Code18.22\n",
        "# 定义训练超参数和训练步骤\n",
        "\n",
        "batch_size = 32\n",
        "discount_rate = 0.95\n",
        "optimizer = keras.optimizers.Adam(lr = 1e-3)\n",
        "loss_fn = keras.losses.mean_squared_error\n",
        "\n",
        "def training_step(batch_size):\n",
        "    # 从重放缓冲区中取一批经验数据\n",
        "    experiences = sample_experiences(batch_size)\n",
        "    # 取出经验中的具体对象\n",
        "    states, actions, rewards, next_states, dones = experiences\n",
        "    # 使用当前模型预测下一个Q值列表\n",
        "    next_Q_values = model.predict(next_states)\n",
        "    # 取出Q值列表中最大的\n",
        "    max_next_Q_values = np.max(next_Q_values, axis = 1)\n",
        "    # 使用公式计算出目标Q值\n",
        "    target_Q_values = (rewards + (1 - dones) * discount_rate * max_next_Q_values)\n",
        "    # 将目标Q值转成一列\n",
        "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
        "    # 掩码\n",
        "    mask = tf.one_hot(actions, n_outputs)\n",
        "    with tf.GradientTape() as tape:\n",
        "        # 计算全部可能动作的Q值\n",
        "        all_Q_values = model(states)\n",
        "        # 只保留经验中的动作的Q值\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis = 1, keepdims = True)\n",
        "        # 使用均方误差作为损失\n",
        "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "    # 计算梯度\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    # 应用梯度下降\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZYe6gbij-FU"
      },
      "source": [
        "# Code18.23\n",
        "# train it!\n",
        "\n",
        "rewards = []\n",
        "best_score = 0\n",
        "\n",
        "for episode in range(600):\n",
        "    obs = env.reset()\n",
        "    for step in range(200):\n",
        "        epsilon = max(1 - episode / 500, 0.01)\n",
        "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
        "        if done:\n",
        "            break\n",
        "    rewards.append(step)\n",
        "    if step > best_score:\n",
        "        best_weights = model.get_weights()\n",
        "        best_score = step\n",
        "    print('\\rEpisode: {}, Steps: {}, eps: {:.3f}'.format(episode, step + 1, epsilon), end = '')\n",
        "    if episode > 50:\n",
        "        training_step(batch_size)\n",
        "\n",
        "model.set_weights(best_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OmVDn_WroC0"
      },
      "source": [
        "# Code18.24\n",
        "# draw it\n",
        "\n",
        "plt.figure(figsize = (8, 4))\n",
        "plt.plot(rewards)\n",
        "plt.xlabel('Episode', fontsize = 14)\n",
        "plt.ylabel('Sum of rewards', fontsize = 14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJj_reCDvqz8"
      },
      "source": [
        "# Code18.25\n",
        "# 看看效果\n",
        "\n",
        "state = env.reset()\n",
        "\n",
        "frames = []\n",
        "\n",
        "for step in range(200):\n",
        "    action = epsilon_greedy_policy(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "        break\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "    frames.append(img)\n",
        "    \n",
        "plot_animation(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFdJ-JP31iTb"
      },
      "source": [
        "# Code18.25\n",
        "# Double DQN\n",
        "\n",
        "# 准备所需要的模型\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(32, activation = 'elu', input_shape = [4]),\n",
        "    keras.layers.Dense(32, activation = 'elu'),\n",
        "    keras.layers.Dense(n_outputs)\n",
        "])\n",
        "\n",
        "target = keras.models.clone_model(model)\n",
        "target.set_weights(model.get_weights())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYvppiEMwu7C"
      },
      "source": [
        "# Code18.26\n",
        "# 定义超参数与训练函数\n",
        "\n",
        "batch_size = 32\n",
        "discount_rate = 0.95\n",
        "optimizer = keras.optimizers.Adam(lr = 1e-3)\n",
        "loss_fn = keras.losses.Huber()\n",
        "\n",
        "def training_step(batch_size):\n",
        "    experiences = sample_experiences(batch_size)\n",
        "    states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "    next_Q_values = model.predict(next_states)\n",
        "    best_next_actions = np.argmax(next_Q_values, axis = 1)\n",
        "    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
        "    next_best_Q_values = (target.predict(next_states) * next_mask).sum(axis = 1)\n",
        "\n",
        "    target_Q_values = (rewards + (1 - dones) * discount_rate * next_best_Q_values)\n",
        "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
        "    mask = tf.one_hot(actions, n_outputs)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q_values = model(states)\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis = 1, keepdims = True)\n",
        "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nME_20r1Glh"
      },
      "source": [
        "# Code18.27\n",
        "# train it\n",
        "\n",
        "replay_memory = deque(maxlen = 2000)\n",
        "rewards = []\n",
        "best_score = 0\n",
        "\n",
        "for episode in range(600):\n",
        "    obs = env.reset()\n",
        "    for step in range(200):\n",
        "        epsilon = max(1 - episode / 500, 0.01)\n",
        "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    rewards.append(step)\n",
        "    if step > best_score:\n",
        "        best_weights = model.get_weights()\n",
        "        best_score = step\n",
        "    print('\\rEpisode: {}, Steps: {}, eps: {:.3f}'.format(episode, step + 1, epsilon), end = '')\n",
        "    if episode > 50:\n",
        "        training_step(batch_size)\n",
        "    if episode % 50 == 0:\n",
        "        target.set_weights(model.get_weights())\n",
        "\n",
        "model.set_weights(best_weights)\n",
        "\n",
        "plt.figure(figsize = (8, 4))\n",
        "plt.plot(rewards)\n",
        "plt.xlabel('Episode', fontsize = 14)\n",
        "plt.ylabel('Sum of rewards', fontsize = 14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY1KT4UFsAQK"
      },
      "source": [
        "# Code18.28\n",
        "# 看看效果\n",
        "\n",
        "state = env.reset()\n",
        "\n",
        "frames = []\n",
        "\n",
        "for step in range(200):\n",
        "    action = epsilon_greedy_policy(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "        break\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "    frames.append(img)\n",
        "    \n",
        "plot_animation(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg6-2UA24F8M"
      },
      "source": [
        "# Code18.29\n",
        "# dueling double dqn\n",
        "\n",
        "K = keras.backend\n",
        "\n",
        "# input layer\n",
        "input_states = keras.layers.Input(shape = [4])\n",
        "# hidden layer\n",
        "hidden1 = keras.layers.Dense(32, activation = 'elu')(input_states)\n",
        "hidden2 = keras.layers.Dense(32, activation = 'elu')(hidden1)\n",
        "\n",
        "state_values = keras.layers.Dense(1)(hidden2)\n",
        "raw_advantages = keras.layers.Dense(n_outputs)(hidden2)\n",
        "advantages = raw_advantages - K.max(raw_advantages, axis = 1, keepdims = True)\n",
        "\n",
        "Q_values = state_values + advantages\n",
        "model = keras.models.Model(inputs = [input_states], outputs = [Q_values])\n",
        "\n",
        "target = keras.models.clone_model(model)\n",
        "target.set_weights(model.get_weights())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUxI5P_i9ln7"
      },
      "source": [
        "# Code18.30\n",
        "# train func\n",
        "\n",
        "batch_size = 32\n",
        "discount_rate = 0.95\n",
        "optimizer = keras.optimizers.Adam(lr = 1e-2)\n",
        "loss_fn = keras.losses.Huber()\n",
        "\n",
        "def training_step(batch_size):\n",
        "    experiences = sample_experiences(batch_size)\n",
        "    states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "    next_Q_values = model.predict(next_states)\n",
        "    best_next_actions = np.argmax(next_Q_values, axis = 1)\n",
        "\n",
        "    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
        "    next_best_Q_values = (target.predict(next_states) * next_mask).sum(axis = 1)\n",
        "    \n",
        "    target_Q_values = (rewards + (1 - dones) * discount_rate * next_best_Q_values)\n",
        "    target_Q_valuse = target_Q_values.reshape(-1, 1)\n",
        "\n",
        "    mask = tf.one_hot(actions, n_outputs)\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q_values = model(states)\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis = 1, keepdims = True)\n",
        "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYTMtGoF_o74"
      },
      "source": [
        "# Code18.31\n",
        "# train it\n",
        "\n",
        "replay_memory = deque(maxlen = 2000)\n",
        "\n",
        "rewards = []\n",
        "best_score = 0\n",
        "\n",
        "for episode in range(600):\n",
        "    obs = env.reset()\n",
        "    for step in range(200):\n",
        "        epsilon = max(1 - episode / 500, 0.01)\n",
        "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    rewards.append(step)\n",
        "    if step > best_score:\n",
        "        best_weights = model.get_weights()\n",
        "        best_score = step\n",
        "    print('\\rEpisode: {}, Steps: {}, wps: {:.3f}'.format(episode, step + 1, epsilon), end = '')\n",
        "\n",
        "    if episode > 50:\n",
        "        training_step(batch_size)\n",
        "    if episode % 200 == 0:\n",
        "        target.set_weights(model.get_weights())\n",
        "\n",
        "model.set_weights(best_weights)\n",
        "\n",
        "plt.plot(rewards)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Sum of rewards')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1IN6SH8CaeQ"
      },
      "source": [
        "# Code18.32\n",
        "# try it\n",
        "\n",
        "state = env.reset()\n",
        "frames = []\n",
        "\n",
        "for step in range(200):\n",
        "    action = epsilon_greedy_policy(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "        break\n",
        "    \n",
        "    img = env.render(mode = 'rgb_array')\n",
        "    frames.append(img)\n",
        "\n",
        "plot_animation(frames)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9IFc41A3llo"
      },
      "source": [
        "# Code18.33\n",
        "# Done!\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXgfWHefwC3v"
      },
      "source": [
        "# 下面这些部分在Colab上运行报错，我就放在本地了\n",
        "# Code18.34\n",
        "# hello tf-agents\n",
        "\n",
        "from tf_agents.environments import suite_gym\n",
        "\n",
        "env = suite_gym.load('Breakout-v4')\n",
        "print(env)\n",
        "print(env.gym)\n",
        "print(env.reset())\n",
        "print(env.step(np.array([1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4QingWOwC3x"
      },
      "source": [
        "# Code18.35\n",
        "# get some other imformation\n",
        "\n",
        "print(env.observation_spec())\n",
        "print(env.action_spec())\n",
        "print(env.time_step_spec())\n",
        "print(env.gym.get_action_meanings())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVRGVHIt9qiO"
      },
      "source": [
        "# Code18.36\n",
        "# use some wrappers\n",
        "\n",
        "from tf_agents.environments.wrappers import ActionRepeat\n",
        "\n",
        "repeating_env = ActionRepeat(env, times = 4)\n",
        "\n",
        "print(repeating_env)\n",
        "repeating_env.unwrapped"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dcrv5WzsAGF8"
      },
      "source": [
        "# Code18.37\n",
        "# wrappers info\n",
        "import tf_agents.environments.wrappers\n",
        "\n",
        "for name in dir(tf_agents.environments.wrappers):\n",
        "    obj = getattr(tf_agents.environments.wrappers, name)\n",
        "    if hasattr(obj, '__base__') and issubclass(obj, tf_agents.environments.wrappers.PyEnvironmentBaseWrapper):\n",
        "        print('{:27s} {}'.format(name, obj.__doc__.split('\\n')[0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErBW-6TeAw45"
      },
      "source": [
        "# Code18.38\n",
        "\n",
        "from functools import partial\n",
        "from gym.wrappers import TimeLimit\n",
        "\n",
        "limited_repeating_env = suite_gym.load('Breakout-v4', gym_env_wrappers = [partial(TimeLimit, max_episode_steps = 10000)], env_wrappers = [partial(ActionRepeat, times = 4)])\n",
        "\n",
        "print(limited_repeating_env)\n",
        "limited_repeating_env.unwrapped"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "919OVCFmC_Kd"
      },
      "source": [
        "# Code18.39\n",
        "\n",
        "from tf_agents.environments import suite_atari\n",
        "from tf_agents.environments.atari_preprocessing import AtariPreprocessing\n",
        "from tf_agents.environments.atari_wrappers import FrameStack4\n",
        "\n",
        "max_episode_steps = 27000\n",
        "environment_name = 'BreakoutNoFrameskip-v4'\n",
        "\n",
        "env = suite_atari.load(environment_name, max_episode_steps = max_episode_steps, gym_env_wrappers = [AtariPreprocessing, FrameStack4])\n",
        "\n",
        "# some example\n",
        "env.seed(42)\n",
        "env.reset()\n",
        "time_step = env.step(np.array([1]))\n",
        "\n",
        "for _ in range(4):\n",
        "    time_step = env.step(np.array([3]))\n",
        "\n",
        "# draw it\n",
        "def plot_observation(obs):\n",
        "    obs = obs.astype(np.float32)\n",
        "    img = obs[..., :3]\n",
        "    current_frame_delta = np.maximum(obs[..., 3] - obs[..., :3].mean(axis = -1), 0.)\n",
        "    img[..., 0] += current_frame_delta\n",
        "    img[..., 2] += current_frame_delta\n",
        "    img = np.clip(img / 150, 0, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.figure(figsize = (6, 6))\n",
        "plot_observation(time_step.observation)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cg187XTCGux4"
      },
      "source": [
        "# Code18.40\n",
        "#\n",
        "\n",
        "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
        "\n",
        "tf_env = TFPyEnvironment(env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2v060ut3idhC"
      },
      "source": [
        "# Code18.41\n",
        "# create DQN\n",
        "\n",
        "from tf_agents.networks.q_network import QNetwork\n",
        "\n",
        "preprocessing_layer = keras.layers.Lambda(lambda obs: tf.cast(obs, np.float32) / 255.0)\n",
        "conv_layer_params = [(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]\n",
        "fc_layer_params = [512]\n",
        "\n",
        "# model DQN\n",
        "q_net = QNetwork(\n",
        "    #\n",
        "    tf_env.observation_spec(),\n",
        "    tf_env.action_spec(),\n",
        "    preprocessing_layers = preprocessing_layer,\n",
        "    conv_layer_params = conv_layer_params,\n",
        "    fc_layer_params = fc_layer_params\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohjrsGZTssaW"
      },
      "source": [
        "# Code18.42\n",
        "# Create DQN agent\n",
        "\n",
        "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
        "\n",
        "train_step = tf.Variable(0)\n",
        "update_period = 4\n",
        "optimizer = tf.compat.v1.train.RMSPropOptimizer(learning_rate = 2.5e-4, decay = 0.95, momentum = 0.0, epsilon = 0.00001, centered = True)\n",
        "\n",
        "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(initial_learning_rate = 1.0, decay_steps = 250000 // update_period, end_learning_rate = 0.01)\n",
        "\n",
        "agent = DqnAgent(\n",
        "    tf_env.time_step_spec(),\n",
        "    tf_env.action_spec(),\n",
        "    q_network = q_net,\n",
        "    optimizer = optimizer,\n",
        "    target_update_period = 2000,\n",
        "    td_errors_loss_fn = keras.losses.Huber(reduction = 'none'),\n",
        "    gamma = 0.99,\n",
        "    train_step_counter = train_step,\n",
        "    epsilon_greedy = lambda: epsilon_fn(train_step)\n",
        ")\n",
        "\n",
        "agent.initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-PTYicGvrIQ"
      },
      "source": [
        "# Code18.43\n",
        "# 建立重放缓冲区\n",
        "\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(data_spec = agent.collect_data_spec, batch_size = tf_env.batch_size, max_length = 1000000)\n",
        "\n",
        "replay_buffer_observer = replay_buffer.add_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yONcClO7wuc2"
      },
      "source": [
        "# Code18.44\n",
        "# create observer\n",
        "\n",
        "class ShowProgress:\n",
        "    def __init__(self, total):\n",
        "        self.counter = 0\n",
        "        self.total = total\n",
        "    \n",
        "    def __call__(self, trajectory):\n",
        "        if not trajectory.is_boundary():\n",
        "            self.counter += 1\n",
        "        if self.counter % 100 == 0:\n",
        "            print('\\r{}/{}'.format(self.counter, self.total), end = '')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RRQdnymyc-x"
      },
      "source": [
        "# Code18.45\n",
        "# create train metrics\n",
        "\n",
        "from tf_agents.metrics import tf_metrics\n",
        "\n",
        "train_metrics = [\n",
        "    # 回合数目\n",
        "    tf_metrics.NumberOfEpisodes(),\n",
        "    # 步骤数目\n",
        "    tf_metrics.EnvironmentSteps(),\n",
        "    # 每回合的平均回报\n",
        "    tf_metrics.AverageReturnMetric(),\n",
        "    # 平均回合的长度\n",
        "    tf_metrics.AverageEpisodeLengthMetric(),\n",
        "]\n",
        "\n",
        "train_metrics[0].result()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKuDNGhZzQaF"
      },
      "source": [
        "# Code18.46\n",
        "# log\n",
        "\n",
        "from tf_agents.eval.metric_utils import log_metrics\n",
        "import logging\n",
        "\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "log_metrics(train_metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mBmNWHw0R03"
      },
      "source": [
        "# Code18.47\n",
        "# create step driver\n",
        "\n",
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
        "\n",
        "collect_driver = DynamicStepDriver(\n",
        "    tf_env,\n",
        "    agent.collect_policy,\n",
        "    observers = [replay_buffer_observer] + train_metrics,\n",
        "    num_steps = update_period\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FN2zRt42MRL"
      },
      "source": [
        "# Code18.48\n",
        "# \n",
        "\n",
        "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
        "\n",
        "initial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(), tf_env.action_spec())\n",
        "init_driver = DynamicStepDriver(\n",
        "    tf_env,\n",
        "    initial_collect_policy,\n",
        "    observers = [replay_buffer.add_batch, ShowProgress(20000)],\n",
        "    num_steps = 20000\n",
        ")\n",
        "\n",
        "final_time_step, final_policy_state = init_driver.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACbQri9u3NRY"
      },
      "source": [
        "# Code18.49\n",
        "# trajectories test\n",
        "\n",
        "tf.random.set_seed(887)\n",
        "\n",
        "trajectories, buffer_info = replay_buffer.get_next(sample_batch_size = 2, num_steps = 3)\n",
        "\n",
        "print(trajectories._fields)\n",
        "print(trajectories.observation.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wfLnSl65MQM"
      },
      "source": [
        "# Code18.50\n",
        "\n",
        "from tf_agents.trajectories.trajectory import to_transition\n",
        "\n",
        "time_steps, action_steps, next_time_steps = to_transition(trajectories)\n",
        "\n",
        "print(time_steps.observation.shape)\n",
        "print(trajectories.step_type.numpy())\n",
        "\n",
        "plt.figure(figsize = (10, 6.8))\n",
        "for row in range(2):\n",
        "    for col in range(3):\n",
        "        plt.subplot(2, 3, row *3 + col + 1)\n",
        "        plot_observation(trajectories.observation[row, col].numpy())\n",
        "plt.subplots_adjust(left = 0, right = 1, bottom = 0, top = 1, hspace = 0, wspace = 0.02)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5OM2bVL7Vvj"
      },
      "source": [
        "# Code18.50\n",
        "# create dataset\n",
        "\n",
        "dataset = replay_buffer.as_dataset(sample_batch_size = 64, num_steps = 2, num_parallel_calls = 3).prefetch(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7iG_Rh97qCL"
      },
      "source": [
        "# Code18.51\n",
        "# train it!\n",
        "\n",
        "from tf_agents.utils.common import function\n",
        "\n",
        "collect_driver.run = function(collect_driver.run)\n",
        "agent.train = function(agent.train)\n",
        "\n",
        "def train_agent(n_iterations):\n",
        "    time_step = None\n",
        "    policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)\n",
        "    iterator = iter(dataset)\n",
        "    for iteration in range(n_iterations):\n",
        "        time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
        "        trajectories, buffer_info = next(iterator)\n",
        "        train_loss = agent.train(trajectories)\n",
        "\n",
        "        print('\\r{} loss: {:.5f}'.format(iteration, train_loss.loss.numpy()), end = '')\n",
        "        if iteration % 1000 == 0:\n",
        "            log_metrics(train_metrics)\n",
        "\n",
        "train_agent(n_iterations = 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uyt9ubKO9_Gk"
      },
      "source": [
        "# Code18.52\n",
        "# draw it!\n",
        "\n",
        "frames = []\n",
        "def save_frames(trajectory):\n",
        "    global frames\n",
        "    frames.append(tf_env.pyenv.envs[0].render(mode = 'rgb_array'))\n",
        "\n",
        "prev_lives = tf_env.pyenv.envs[0].ale.lives()\n",
        "def reset_and_fire_on_life_lost(trajectory):\n",
        "    global prev_lives\n",
        "    lives = tf_env.pyenv.envs[0].ale.lives()\n",
        "    if prev_lives != lives:\n",
        "        tf_env.reset()\n",
        "        tf_env.pyenv.envs[0].step(np.array([1]))\n",
        "        prev_lives = lives\n",
        "\n",
        "watch_driver = DynamicStepDriver(\n",
        "    tf_env,\n",
        "    agent.policy,\n",
        "    observers = [save_frames, reset_and_fire_on_life_lost, ShowProgress(1000)],\n",
        "    num_steps = 1000\n",
        ")\n",
        "\n",
        "final_time_step, final_policy_state = watch_driver.run()\n",
        "\n",
        "plot_animation(frames)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}