# 第4章 训练模型

---

### 练习题
---

###### 1. 如果训练集有数百万个特征，那么可以是用哪种线性回归训练算法？
> 特征太多可以选择随机梯度下降算法或批量梯度下降算法。
---
###### 2. 如果训练集里特征的数值大小迥异，哪种算法可能受到影响？受影响程度如何？你应该怎么做？
> 各种的梯度下降算法，会延长收敛时间，但最终也会收敛，可以再训练之前对数据进行标准化。
---
###### 3. 训练逻辑回归模型时，梯度下降会卡在局部最小值吗？
> 不会，因为成本函数是个完全的凸函数
---
###### 4. 如果你让他们运行足够长的时间，是否所有梯度下降算法都能得出相同的模型？
> 不一定，因为还有其它参数的影响，批量梯度下降有可能发散或停滞，而随机梯度下降可以跳出这些限制。
---
###### 5. 假设你使用批量梯度下降，并在每个轮次绘制验证误差。如果你发现验证错误持续上升，可能是什么情况？你该如何解决？
> 有可能是因为学习率过高导致发散了，可以尝试降低学习率。
---
###### 6. 当验证错误上升时立即停止小批量梯度下降是个好注意吗？
> 不是，小批量梯度下降在训练开始的几个伦次会因为尝试新的数据而导致过高的验证错误。
---
###### 7.哪种梯度下降算法（在我们讨论过的算法中）将最快达到最佳解附近？哪个实际上会收敛？如何使其它也收敛？
> 随机梯度下降会最快达到最佳解附近，批量梯度下降会收敛，想要其它两个也收敛，随着迭代的过程降低学习率就好。
---
###### 8. 假设你正在使用多项式回归。绘制学习曲线后，你会发现训练误差和验证误差之间存在很大差距。发生了什么？解决此问题的三种方法是什么？
>  有可能是因为过拟合了。可以选择降低模型参数复杂度，正则化，增加训练数据集。
---
###### 9.假设你正在使用岭回归，并且你注意到训练误差和验证误差几乎相等且相当高。你是否会说模型存在高方差或者高偏差？你应该增加正则化超参数alpha还是减小它呢？
> 这个是有可能发生了欠拟合，高偏差，应该减小正则化超参数的值。
---
###### 10.1. 为什么要使用岭回归而不是简单地线性回归（即没有任何的正则化）？
> 正则化可以有效地避免过拟合，而且可以调节正则化影响强度，所以说有总比没有要好。
---
###### 10.2. 为什么要使用Lasso而不是岭回归？
> Lasso可以强调某些特征，并列将某些特征影响的效果降为0，对提取高度相关特征与去除无效特征（如果存在）有很大好处。
---
###### 10.3. 为什么要使用弹性网络而不是Lasso？
> 弹性网络有更好的灵活性。
---
###### 11. 假设你要将图片分类为室内/室外和白天/夜间。你应该实现两个逻辑回归分类器还是一个Softmax分类器？
> 因为这两组类型之间不互斥或者说相互独立，所以可以使用两个逻辑回归分类器。
---
###### 12. 用Softmax回归进行批量梯度下降训练，实现提前停止法（不使用Scikit-Learn）
> 未完成
