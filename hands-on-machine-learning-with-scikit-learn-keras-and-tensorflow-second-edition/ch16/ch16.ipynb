{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "ch16.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7o77xxhBvuV"
      },
      "source": [
        "# 2020-10-30 created by Akson"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_4aM1r4Bx2j"
      },
      "source": [
        "# Code16.1\n",
        "# import\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GylF_6uBzPX"
      },
      "source": [
        "# Code16.2\n",
        "# download txt\n",
        "\n",
        "shakespeare_url = 'https://homl.info/shakespeare'\n",
        "filepath = keras.utils.get_file('shakespeare.txt', shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "    shakespeare_text = f.read()\n",
        "\n",
        "print(shakespeare_text[:148])\n",
        "print(''.join(sorted(set(shakespeare_text.lower()))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxfrtwLzBznP"
      },
      "source": [
        "# Code16.3\n",
        "# encode\n",
        "\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level = True)\n",
        "tokenizer.fit_on_texts(shakespeare_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhWr3PLOBzve"
      },
      "source": [
        "# Code16.4\n",
        "# test\n",
        "\n",
        "print(tokenizer.texts_to_sequences(['First']))\n",
        "print(tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]]))\n",
        "\n",
        "max_id = len(tokenizer.word_index)\n",
        "dataset_size = tokenizer.document_count\n",
        "\n",
        "print(max_id)\n",
        "print(dataset_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYbW89QtBz12"
      },
      "source": [
        "# Code16.5\n",
        "# \n",
        "\n",
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV3qoDa-Bz8i"
      },
      "source": [
        "# Code16.6\n",
        "\n",
        "train_size = dataset_size * 90 // 100\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYzOkKfKLmi6"
      },
      "source": [
        "# Code16.7\n",
        "# window\n",
        "\n",
        "n_steps = 100\n",
        "window_length = n_steps + 1\n",
        "dataset = dataset.repeat().window(window_length, shift = 1, drop_remainder = True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "\n",
        "batch_size = 32\n",
        "# 混洗，并分成每批32个\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "# 将前一百个字符与最后一个目标字符分开\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "# 分别对训练集与目标集进行独热编码\n",
        "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth = max_id), Y_batch))\n",
        "# 预取\n",
        "dataset = dataset.prefetch(1)\n",
        "\n",
        "for X_batch, Y_batch in dataset.take(1):\n",
        "    print(X_batch.shape, Y_batch.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H90MMeaVLmxn"
      },
      "source": [
        "# Code16.8\n",
        "# train\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences = True, input_shape = [None, max_id], dropout = 0.2, recurrent_dropout = 0.2),\n",
        "    keras.layers.GRU(128, return_sequences = True, dropout = 0.2, recurrent_dropout = 0.2),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation = 'softmax'))\n",
        "])\n",
        "\n",
        "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam')\n",
        "history = model.fit(dataset, steps_per_epoch = train_size // batch_size, epochs = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK9hJNh8eV4f"
      },
      "source": [
        "# Code16.9\n",
        "# use model\n",
        "\n",
        "def preprocess(texts):\n",
        "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
        "    return tf.one_hot(X, max_id)\n",
        "\n",
        "X_new = preprocess(['How are yo'])\n",
        "y_pred = model.predict_classes(X_new)\n",
        "print(tokenizer.sequences_to_texts(y_pred + 1)[0][-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l_S_uKZfJby"
      },
      "source": [
        "# Code16.10\n",
        "\n",
        "def next_char(text, model, temperature = 1):\n",
        "    X_new = preprocess([text])\n",
        "    y_proba = model.predict(X_new)[0, -1:, :]\n",
        "    rescaled_logists = tf.math.log(y_proba) / temperature\n",
        "    char_id = tf.random.categorical(rescaled_logists, num_samples = 1) + 1\n",
        "\n",
        "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
        "\n",
        "def complete_text(text, model, n_chars = 50, temperature = 1):\n",
        "    for _ in range(n_chars):\n",
        "        text += next_char(text, model, temperature)\n",
        "    return text\n",
        "\n",
        "print(complete_text('t', model, temperature = 0.2))\n",
        "print(complete_text('w', model, temperature = 1))\n",
        "print(complete_text('w', model, temperature = 2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n28TuJHvl2JK"
      },
      "source": [
        "# Code16.11\n",
        "# dataset prepare\n",
        "\n",
        "# 设置每一批的数量\n",
        "batch_size = 32\n",
        "\n",
        "n_steps = 100\n",
        "window_length = n_steps + 1\n",
        "# 取出要训练的部分，分成32份\n",
        "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
        "# 建空数据集\n",
        "datasets = []\n",
        "# 对于每一批次的数据\n",
        "for encoded_part in encoded_parts:\n",
        "    # 数据切片\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
        "    # 设置窗口\n",
        "    dataset = dataset.window(window_length, shift = n_steps, drop_remainder = True)\n",
        "    # 将数据窗口内的数据展开\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "    # 将本轮数据加入数据集中\n",
        "    datasets.append(dataset)\n",
        "\n",
        "# 打包\n",
        "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
        "# 拆分训练集与目标数据集\n",
        "dataset = dataset.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "# 再将训练用数据集转换为独热码\n",
        "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "# 预取\n",
        "dataset = dataset.prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79CXwUZwpIzH"
      },
      "source": [
        "# Code16.12\n",
        "# state model prepare\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences = True, stateful = True, dropout = 0.2, recurrent_dropout = 0.2, batch_input_shape = [batch_size, None, max_id]),\n",
        "    keras.layers.GRU(128, return_sequences = True, stateful = True, dropout = 0.2, recurrent_dropout = 0.2),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation = 'softmax'))\n",
        "])\n",
        "\n",
        "class ResetStatesCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs):\n",
        "        self.model.reset_states()\n",
        "\n",
        "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam')\n",
        "model.fit(dataset, steps_per_epoch = train_size // batch_size // n_steps, epochs = 35, callbacks = [ResetStatesCallback()])\n",
        "\n",
        "stateless_model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences = True, input_shape = [None, max_id]),\n",
        "    keras.layers.GRU(128, return_sequences = True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation = 'softmax'))\n",
        "])\n",
        "\n",
        "stateless_model.build(tf.TensorShape([None, None, max_id]))\n",
        "stateless_model.set_weights(model.get_weights())\n",
        "print(complete_text('t', stateless_model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhYCJsV-sGlT"
      },
      "source": [
        "# Code16.13\n",
        "# keras.imdb test\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()\n",
        "print(X_train[0][:10])\n",
        "\n",
        "word_index = keras.datasets.imdb.get_word_index()\n",
        "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
        "for id_, token in enumerate(('<pad>', '<sos>', '<unk>')):\n",
        "    id_to_word[id_] = token\n",
        "' '.join([id_to_word[id_] for id_ in X_train[0][:10]])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4DkIWJ_sGtL"
      },
      "source": [
        "# Code16.14\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "from collections import Counter\n",
        "\n",
        "datasets, info = tfds.load('imdb_reviews', as_supervised = True, with_info = True, try_gcs = True)\n",
        "train_size = info.splits['train'].num_examples\n",
        "\n",
        "def preprocess(X_batch, y_batch):\n",
        "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
        "    X_batch = tf.strings.regex_replace(X_batch, b'<br\\\\s*/?>', b' ')\n",
        "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b' ')\n",
        "    X_batch = tf.strings.split(X_batch)\n",
        "    \n",
        "    return X_batch.to_tensor(default_value = b'<pad>'), y_batch\n",
        "\n",
        "vocabulary = Counter()\n",
        "for X_batch, y_batch in datasets['train'].batch(32).map(preprocess):\n",
        "    for review in X_batch:\n",
        "        vocabulary.update(list(review.numpy()))\n",
        "\n",
        "# print top 3 words\n",
        "print(vocabulary.most_common()[:3])\n",
        "vocab_size = 10000\n",
        "truncated_vocabulary = [word for word, count in vocabulary.most_common()[:vocab_size]]\n",
        "\n",
        "words = tf.constant(truncated_vocabulary)\n",
        "word_ids = tf.range(len(truncated_vocabulary), dtype = tf.int64)\n",
        "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
        "num_oov_buckets = 1000\n",
        "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)\n",
        "\n",
        "# some words' id\n",
        "table.lookup(tf.constant([b'This is movie was faaaaaantastic'.split()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxowTQHOsGyw"
      },
      "source": [
        "# Code16.15\n",
        "# create train set\n",
        "\n",
        "def encode_words(X_batch, y_batch):\n",
        "    return table.lookup(X_batch), y_batch\n",
        "\n",
        "train_set = datasets['train'].repeat().batch(32).map(preprocess)\n",
        "train_set = train_set.map(encode_words).prefetch(1)\n",
        "\n",
        "embed_size = 128\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size, input_shape = [None]),\n",
        "    keras.layers.GRU(128, return_sequences = True),\n",
        "    keras.layers.GRU(128),\n",
        "    keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "history = model.fit(train_set, steps_per_epoch = train_size // 32, epochs = 5)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sfFL0PjsG4i"
      },
      "source": [
        "# Code16.16\n",
        "# mask\n",
        "\n",
        "K = keras.backend\n",
        "embed_size = 128\n",
        "\n",
        "# layer\n",
        "inputs = keras.layers.Input(shape = [None])\n",
        "# create mask layer\n",
        "mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
        "z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
        "z = keras.layers.GRU(128, return_sequences = True)(z, mask = mask)\n",
        "z = keras.layers.GRU(128)(z, mask = mask)\n",
        "outputs = keras.layers.Dense(1, activation = 'sigmoid')(z)\n",
        "\n",
        "model = keras.models.Model(inputs = [inputs], outputs = [outputs])\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "hittory = model.fit(train_set, steps_per_epoch = train_size // 32, epochs = 5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wklIax7wSC3L"
      },
      "source": [
        "# Code16.17\n",
        "# tf hub\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "datasets, info = tfds.load('imdb_reviews', as_supervised = True, with_info = True, try_gcs = True)\n",
        "train_size = info.splits['train'].num_examples\n",
        "batch_size = 32\n",
        "train_set = datasets['train'].repeat().batch(batch_size).prefetch(1)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    hub.KerasLayer('https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1', dtype = tf.string, input_shape = [], output_shape = [50]),\n",
        "    keras.layers.Dense(128, activation = 'relu'),\n",
        "    keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "history = model.fit(train_set, steps_per_epoch = train_size // batch_size, epochs = 5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pt7sRburSDAn"
      },
      "source": [
        "# Code16.18\n",
        "# encoder and decoder\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "encoder_inputs = keras.layers.Input(shape = [None], dtype = np.int32)\n",
        "decoder_inputs = keras.layers.Input(shape = [None], dtype = np.int32)\n",
        "sequence_lengths = keras.layers.Input(shape = [], dtype = np.int32)\n",
        "\n",
        "vocab_size = 100\n",
        "embed_size = 10\n",
        "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
        "encoder_embeddings = embeddings(encoder_inputs)\n",
        "decoder_embeddings = embeddings(decoder_inputs)\n",
        "\n",
        "encoder = keras.layers.LSTM(512, return_state = True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
        "encoder_state = [state_h, state_c]\n",
        "\n",
        "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "\n",
        "decoder_cell = keras.layers.LSTMCell(512)\n",
        "output_layer = keras.layers.Dense(vocab_size)\n",
        "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler, output_layer = output_layer)\n",
        "\n",
        "final_outputs, final_state, final_sequence_lengths = decoder(decoder_embeddings, initial_state = encoder_state, sequence_length = sequence_lengths)\n",
        "y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
        "\n",
        "model = keras.Model(inputs = [encoder_inputs, decoder_inputs, sequence_lengths], outputs = [y_proba])\n",
        "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam')\n",
        "\n",
        "X = np.random.randint(100, size = 10 * 1000).reshape(1000, 10)\n",
        "y = np.random.randint(100, size = 15 * 1000).reshape(1000, 15)\n",
        "X_decoder = np.c_[np.zeros((1000, 1)), y[:, : -1]]\n",
        "seq_lengths = np.full([1000], 15)\n",
        "\n",
        "history = model.fit([X, X_decoder, seq_lengths], y, epochs = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuZeGCxpSDHC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIg4OtSGSDZk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}