{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2020-10-26 created by Akson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code11.1\n",
    "# 加载所需要的数据集\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code11.2\n",
    "# 批量归一化建模方法\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation = 'elu', kernel_initializer = 'he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation = 'elu', kernel_initializer = 'he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# 查看第一个bn层的参数\n",
    "for var in model.layers[1].variables:\n",
    "    print(var.name, var.trainable)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code11.3\n",
    "# 在隐藏层和激活层之间使用批量归一化\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, kernel_initializer = 'he_normal', use_bias = False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('elu'),\n",
    "    keras.layers.Dense(100, kernel_initializer = 'he_normal', use_bias = False),\n",
    "    keras.layers.Activation('elu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code11.4\n",
    "# 查看BatchNormalization的源代码\n",
    "\n",
    "# print(keras.layers.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code11.5\n",
    "# 梯度裁剪的设置方法\n",
    "\n",
    "# 在设置最优化算法时加这个参数即可\n",
    "# optimizer = keras.optimizers.SGD(clipvalue = 1.0) # 只对处于阈值之外的值进行缩放，会改变方向\n",
    "# optimizer = keras.optimizers.SGD(clipnorm = 1.0) # 保留方向，但有可能消除较小值的影响\n",
    "# model.compile(loss = 'mse', optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code11.6\n",
    "# 拆分数据集\n",
    "\n",
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code11.7\n",
    "\n",
    "# 训练一个模型\n",
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))\n",
    "\n",
    "model_A.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.SGD(lr=1e-3), metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_A.fit(X_train_A, y_train_A, epochs=20, validation_data=(X_valid_A, y_valid_A))\n",
    "\n",
    "# 保存模型\n",
    "model_A.save(\"my_model_A.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code11.8\n",
    "# 尝试重用已有的模型\n",
    "\n",
    "model_A = keras.models.load_model('my_model_A.h5')\n",
    "# 为了避免直接修改模型A，先创建一个A的副本\n",
    "# 克隆A的架构\n",
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "# 设置权重\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "\n",
    "# 重用A的某几层（一般是底层）\n",
    "model_B_on_A = keras.models.Sequential(model_A_clone.layers[:-1])\n",
    "# 再添加自己的一些东西\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "# 为了避免破坏重用的权重，先把重用的层冻结\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model_B_on_A.compile(loss = 'binary_crossentropy', optimizer = 'sgd', metrics = ['accuracy'])\n",
    "# 先训练几次\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs = 4, validation_data = (X_valid_B, y_valid_B))\n",
    "\n",
    "# 解冻\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# 在训练几次，这一次记得要调小最优化算法的学习率\n",
    "optimizer = keras.optimizers.SGD(lr = 1e-4)\n",
    "model_B_on_A.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "# 在接着训练\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs = 16, validation_data = (X_valid_B, y_valid_B))\n",
    "\n",
    "print(model_B_on_A.evaluate(X_test_B, y_test_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code11.9\n",
    "# 动量优化\n",
    "\n",
    "# 只要改变这个参数就好\n",
    "# optimizer = keras.optimizers.SGD(lr = 0.001, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code11.10\n",
    "# Nesterov加速梯度\n",
    "\n",
    "# 只要改变这个参数就好\n",
    "# optimizer = keras.optimizers.SGD(lr = 0.001, momentum = 0.9, nesterov = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code11.11\n",
    "# RMSProp\n",
    "\n",
    "# 只要改变这个参数就好\n",
    "# optimizer = keras.optimizers.RMSprop(lr = 0.001, rho = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code11.12\n",
    "# Adam\n",
    "\n",
    "# 只要改变这个参数就好\n",
    "# optimizer = keras.optimizers.Adma(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code11.13\n",
    "# 学习率调度\n",
    "\n",
    "# 幂调度\n",
    "# optimizer = keras.optimizers.SGD(lr = 0.01, decay = 1e-4)\n",
    "\n",
    "# 指数调度，要注意使用epoch时如果有中断训练的情况可能会变复杂\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1 **(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "# exponential_decay_fn = exponential_decay(lr0 = 0.01, s = 20)\n",
    "\n",
    "# lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "# history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid), callbacks=[lr_scheduler])\n",
    "\n",
    "# 将学习率作为参数\n",
    "def exponential_decay_fn(epoch, lr):\n",
    "    return lr * 0.1 ** (1 / 20)\n",
    "\n",
    "# 分段恒定调度\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001\n",
    "\n",
    "# 性能调度（如果性能连续几个伦次不变的话，改变学习率）\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor = 0.5, patience = 5)\n",
    "\n",
    "# 可以使用调度方法来定义学习率，然后将学习率再传给优化器\n",
    "# s = 20 * len(X_train)\n",
    "# learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "# optimizer = keras.optimizers.SGD(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code11.14\n",
    "# l1 l2 正则化\n",
    "\n",
    "layer = keras.layers.Dense(100, activation = 'elu', kernel_initializer = 'he_normal', kernel_regularizer = keras.regularizers.l2(0.01))\n",
    "\n",
    "# 使用循环来重构代码或使用partial来打包某层的格式\n",
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense, activation = 'elu', kernel_initializer = 'he_normal', kernel_regularizer = keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation = 'softmax', kernel_initializer = 'glorot_uniform')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code11.15\n",
    "# Dropout\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    keras.layers.Dropout(rate = 0.2),\n",
    "    keras.layers.Dense(300, activation = 'elu', kernel_initializer = 'he_normal'),\n",
    "    keras.layers.Dropout(rate = 0.2),\n",
    "    keras.layers.Dense(100, activation = 'elu', kernel_initializer = 'he_normal'),\n",
    "    keras.layers.Dropout(rate = 0.2),\n",
    "    keras.layers.Dense(10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 5\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code11.16\n",
    "# MC Dropout\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
