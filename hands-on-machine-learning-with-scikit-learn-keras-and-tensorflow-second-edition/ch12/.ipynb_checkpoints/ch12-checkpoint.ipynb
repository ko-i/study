{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2020-10-27 created by Akson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.2\n",
    "# 引入需要的包\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.2\n",
    "# 初始化张量\n",
    "\n",
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.3\n",
    "# 初始化标量\n",
    "\n",
    "scalar = tf.constant(42)\n",
    "scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.4\n",
    "# 形状\n",
    "\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.5\n",
    "# 数据类型\n",
    "\n",
    "t.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.6\n",
    "# 索引1\n",
    "\n",
    "t[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.7\n",
    "# 索引2\n",
    "\n",
    "t[..., 1, tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.8\n",
    "# 相加\n",
    "\n",
    "t + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.9\n",
    "# square\n",
    "\n",
    "tf.square(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.10\n",
    "# matrix multiply and transpose\n",
    "\n",
    "t @ tf.transpose(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.11\n",
    "# simple keras example\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "K = keras.backend\n",
    "K.square(K.transpose(t)) + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.12\n",
    "# Tensor and numpy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "a = np.array([2., 4., 5.])\n",
    "tf.constant(a, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.13\n",
    "# Tensor and numpy\n",
    "\n",
    "# np.array(t)\n",
    "t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.14\n",
    "# Tensor and numpy\n",
    "\n",
    "tf.square(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.15\n",
    "# Tensor and numpy\n",
    "\n",
    "np.square(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.16\n",
    "# 运算时数据类型需要保持一致\n",
    "\n",
    "# tf.constant(2) + tf.constant(3.0)\n",
    "# tf.constant(2.0) + tf.constant(3.0, dtype = tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.17\n",
    "# 我们用到的其实是这个，变量\n",
    "\n",
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "print(v)\n",
    "print(v.assign(2 * v))\n",
    "print(v[0, 1].assign(42))\n",
    "print(v[:, 2].assign([0., 1.]))\n",
    "print(v.scatter_nd_update(indices = [[0, 0], [1, 2]], updates = [100., 200.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.18\n",
    "# 自定义损失函数，实现huber损失\n",
    "\n",
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.19\n",
    "# load dataset\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state = 42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.20\n",
    "# use huber_fn\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = \"selu\", kernel_initializer = \"lecun_normal\", input_shape = input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss = huber_fn, optimizer = 'nadam', metrics = 'mae')\n",
    "history = model.fit(X_train, y_train, epochs = 2, validation_data = (X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.21\n",
    "# save and load model\n",
    "\n",
    "model.save('my_model_with_a_custom_loss.h5')\n",
    "model = keras.models.load_model('my_model_with_a_custom_loss.h5', custom_objects = {'huber_fn': huber_fn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.22\n",
    "# use different threshold\n",
    "\n",
    "def create_huber(threshold = 1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < 1\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = threshold * tf.abs(error) - threshold ** 2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = 'selu', kernel_initializer = 'lecun_normal', input_shape = input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss = create_huber(2.0), optimizer = 'nadam', metrics = 'mae')\n",
    "history = model.fit(X_train, y_train, epochs = 2, validation_data = (X_valid_scaled, y_valid))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.23\n",
    "# save and load model\n",
    "\n",
    "model.save('my_model_with_a_custom_loss.h5')\n",
    "model = keras.models.load_model('my_model_with_a_custom_loss.h5', custom_objects = {'huber_fn': create_huber(2.0)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.24\n",
    "# 直接继承keras的损失函数类是个很棒的方法\n",
    "\n",
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold = 1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < 1\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold ** 2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'threshold': self.threshold}\n",
    "    \n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = 'selu', kernel_initializer = 'lecun_normal', input_shape = input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss = HuberLoss(2.0), optimizer = 'nadam', metrics = 'mae')\n",
    "history = model.fit(X_train, y_train, epochs = 2, validation_data = (X_valid_scaled, y_valid))\n",
    "\n",
    "# 这次就可以比较方便的存储和加载模型了\n",
    "model.save('my_model_with_a_custom_loss.h5')\n",
    "model = keras.models.load_model('my_model_with_a_custom_loss.h5', custom_objects = {'HuberLoss': HuberLoss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.25\n",
    "# 自定义其它\n",
    "\n",
    "# custom activation\n",
    "# equal to keras.activations.softplus() or tf.nn.softplus()\n",
    "def my_softplus(z):\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "# custom initializer\n",
    "# equal to keras.initializers.glorot_normal()\n",
    "def my_glorot_initializer(shape, dtype = tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev = stddev, dtype = dtype)\n",
    "\n",
    "# custom regularizer\n",
    "# equal to keras.regularizers.l1(0.01)\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "# custom contraint\n",
    "# equal to keras.contraints.nonneg() or tf.nn.relu()\n",
    "def my_positive_weights(weights):\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = 'selu', kernel_initializer = 'lecun_normal', input_shape = input_shape),\n",
    "    keras.layers.Dense(1, activation = my_softplus, kernel_initializer = my_glorot_initializer, kernel_regularizer = my_l1_regularizer, kernel_constraint = my_positive_weights),\n",
    "])\n",
    "\n",
    "model.compile(loss = 'mse', optimizer = 'nadam', metrics = 'mae')\n",
    "history = model.fit(X_train_scaled, y_train, epochs = 2, validation_data = (X_valid_scaled, y_valid))\n",
    "\n",
    "model.save(\"my_model_with_many_custom_parts.h5\")\n",
    "model = keras.models.load_model( \"my_model_with_many_custom_parts.h5\", custom_objects={\"my_l1_regularizer\": my_l1_regularizer, \"my_positive_weights\": my_positive_weights, \"my_glorot_initializer\": my_glorot_initializer, \"my_softplus\": my_softplus})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.26\n",
    "# l1正则化继承类表示\n",
    "\n",
    "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    \n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {'factor': self.factor}\n",
    "    \n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = 'selu', kernel_initializer = 'lecun_normal', input_shape = input_shape),\n",
    "    keras.layers.Dense(1, activation = my_softplus, kernel_initializer = my_glorot_initializer, kernel_regularizer = MyL1Regularizer(0.01), kernel_constraint = my_positive_weights),\n",
    "])\n",
    "\n",
    "model.compile(loss = 'mse', optimizer = 'nadam', metrics = 'mae')\n",
    "history = model.fit(X_train_scaled, y_train, epochs = 2, validation_data = (X_valid_scaled, y_valid))\n",
    "\n",
    "model.save(\"my_model_with_many_custom_parts.h5\")\n",
    "model = keras.models.load_model( \"my_model_with_many_custom_parts.h5\", custom_objects={\"MyL1Regularizer\": MyL1Regularizer, \"my_positive_weights\": my_positive_weights, \"my_glorot_initializer\": my_glorot_initializer, \"my_softplus\": my_softplus})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.27\n",
    "# use huber as metrics\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = 'selu', kernel_initializer = 'lecun_normal', input_shape = input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss = 'mse', optimizer = 'nadam', metrics = [create_huber(2.0)])\n",
    "history = model.fit(X_train, y_train, epochs = 2, validation_data = (X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.28\n",
    "# Precision test\n",
    "\n",
    "precision = keras.metrics.Precision()\n",
    "print(precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1]))\n",
    "print(precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0]))\n",
    "\n",
    "print(precision.result())\n",
    "print(precision.variables)\n",
    "print(precision.reset_states())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.29\n",
    "# 封装成类\n",
    "\n",
    "class HuberMetric(keras.metrics.Metric):\n",
    "    def __init__(self, threshold = 1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        self.total = self.add_weight('total', initializer = 'zeros')\n",
    "        self.count = self.add_weight('count', initializer = 'zeros')\n",
    "    \n",
    "    def huber_fn(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight = None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "        \n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'threshold': self.threshold}\n",
    "    \n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = 'selu', kernel_initializer = 'lecun_normal', input_shape = input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss = 'mse', optimizer = 'nadam', metrics = [HuberMetric(1.0)])\n",
    "history = model.fit(X_train, y_train, epochs = 2, validation_data = (X_valid_scaled, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.30\n",
    "# custom layer\n",
    "\n",
    "exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.31\n",
    "# simple Dense\n",
    "\n",
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation = None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        \n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(name = 'kernel', shape = [batch_input_shape[-1], self.units], initializer = 'zeros')\n",
    "        self.bias = self.add_weight(name = 'bias', shape = [self.units], initializer = 'zeros')\n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "    \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'units': self.units, 'activation': keras.activations.serialize(self.activation)}\n",
    "    \n",
    "model = keras.models.Sequential([\n",
    "    MyDense(30, activation=\"relu\", input_shape=input_shape),\n",
    "    MyDense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "model.save(\"my_model_with_a_custom_layer.h5\")\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_layer.h5\", custom_objects={\"MyDense\": MyDense})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.32\n",
    "# 多输入多输出\n",
    "\n",
    "class MyMultiLayer(keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        X1, X2 = X\n",
    "        return [X1 + X2, X1 * X2, X1 / X2]\n",
    "    \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        b1, b2 = batch_input_shape\n",
    "        return [b1, b1, b1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.33\n",
    "# 如果层的行为在训练和测试期间不同呢\n",
    "\n",
    "class MyGaussianNoise(keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "    \n",
    "    def call(self, X, training = None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev = self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.34\n",
    "# custom model\n",
    "\n",
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(n_neurons, activation = 'elu', kernel_initializer = 'he_normal') for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z\n",
    "    \n",
    "\n",
    "class ResidualRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.hidden1 = keras.layers.Dense(30, activation = 'elu', kernel_initializer = 'he_normal')\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3):\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)\n",
    "\n",
    "model = ResidualRegressor(1)\n",
    "model.compile(loss = 'mse', optimizer = 'nadam')\n",
    "history = model.fit(X_train_scaled, y_train, epochs = 5)\n",
    "X_new_scaled = X_test_scaled\n",
    "score = model.evaluate(X_test_scaled, y_test)\n",
    "y_pred = model.predict(X_new_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.35\n",
    "# 模型内部的损失\n",
    "\n",
    "class ReconstructingRegressor(keras.models.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30, activation = 'selu', kernel_initializer = 'lecun_normal') for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        self.reconstruction_mean = keras.metrics.Mean(name=\"reconstruction_error\")\n",
    "        \n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        # self.add_loss(0.05 * recon_loss)\n",
    "        \n",
    "        result = self.reconstruction_mean(recon_loss)\n",
    "        self.add_metric(result)\n",
    "        \n",
    "        return self.out(Z)\n",
    "    \n",
    "model = ReconstructingRegressor(1)\n",
    "model.compile(loss= 'mse', optimizer = 'nadam')\n",
    "history = model.fit(X_train_scaled, y_train, epochs = 2)\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.36\n",
    "# auto-diff\n",
    "\n",
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2\n",
    "\n",
    "w1, w2 = 5, 3\n",
    "eps = 1e-6\n",
    "print((f(w1 + eps, w2) - f(w1, w2)) / eps)\n",
    "print((f(w1 , w2 + eps) - f(w1, w2)) / eps)\n",
    "\n",
    "\n",
    "# 使用tf来处理微分\n",
    "w1 = tf.Variable(5.)\n",
    "w2 = tf.Variable(3.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "    \n",
    "# 只能调用一次tape.gradient\n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "print(gradients)\n",
    "\n",
    "\n",
    "# 多次使用tape.gradient\n",
    "with tf.GradientTape(persistent = True) as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "dz_dw2 = tape.gradient(z, w2)\n",
    "del tape\n",
    "\n",
    "\n",
    "# 尝试对非变量进行操作\n",
    "c1 = tf.constant(5.)\n",
    "c2 = tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(c1, c2)\n",
    "    \n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "print(gradients)\n",
    "\n",
    "\n",
    "# 我非要对非变量进行操作\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "    \n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "print(gradients)\n",
    "\n",
    "\n",
    "# 阻止某些权重执行微分\n",
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "    \n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.37\n",
    "# 在执行softplus函数计算其梯度时带入大数可能会引起异常\n",
    "\n",
    "x = tf.Variable([100.])\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_softplus(x)\n",
    "    \n",
    "tape.gradient(z, [x])\n",
    "\n",
    "@tf.custom_gradient\n",
    "def my_better_softplus(z):\n",
    "    exp = tf.exp(z)\n",
    "    def my_softplus_gradients(grad):\n",
    "        return grad / (1 + 1 / exp)\n",
    "    return tf.math.log(exp + 1), my_softplus_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.38\n",
    "# 自定义循环（构建一个模型）\n",
    "\n",
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = 'elu', kernel_initializer = 'he_normal', kernel_regularizer = l2_reg),\n",
    "    keras.layers.Dense(1, kernel_regularizer = l2_reg)\n",
    "])\n",
    "\n",
    "# 从训练集中随机采样的函数\n",
    "def random_batch(X, y, batch_size = 32):\n",
    "    idx = np.random.randint(len(X), size = batch_size)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "# 显示训练状态的函数\n",
    "def print_status_bar(iteration, total, loss, metrics = None):\n",
    "    metrics = ' - '.join(['{}: {:.4f}'.format(m.name, m.result()) for m in [loss] + (metrics or [])])\n",
    "    end = '' if iteration < total else '\\n'\n",
    "    print('\\r{}/{} - '.format(iteration, total) + metrics, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.39\n",
    "# 构建循环！\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train)\n",
    "optimizer = keras.optimizers.Nadam(lr = 0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]\n",
    "\n",
    "# 轮次循环\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print('Epoch {}/{}'.format(epoch, n_epochs))\n",
    "    \n",
    "    # 轮次内的批处理循环\n",
    "    for step in range(1, n_steps + 1):\n",
    "        # 从训练集中抽取随机的批次\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 对这批数据进行预测\n",
    "            y_pred = model(X_batch, training = True)\n",
    "            # 主要的损失（均值）\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            # 总损失=主要损失+其他损失（正则化损失之类）\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        # 针对每个可训练变量求梯度\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        # 运用优化器来执行梯度下降步骤\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        # 更新损失和指标\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        # 打印本批次的状态\n",
    "        # print_status_bar(step, len(y_train), mean_loss, metrics)\n",
    "    # 打印本轮次的状态\n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
    "    # 重置损失和指标状态\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code12.40\n",
    "\n",
    "def cube(x):\n",
    "    return x ** 3\n",
    "\n",
    "print(cube(2))\n",
    "print(cube(tf.constant(2.0)))\n",
    "\n",
    "tf_cube = tf.function(cube)\n",
    "print(tf_cube)\n",
    "print(tf_cube(2))\n",
    "print(tf_cube(tf.constant(2.0)))\n",
    "\n",
    "@tf.function\n",
    "def tf_cube(x):\n",
    "    return x ** 3\n",
    "\n",
    "print(tf_cube.python_function(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
