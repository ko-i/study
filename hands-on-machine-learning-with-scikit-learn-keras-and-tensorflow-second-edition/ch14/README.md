# 第14章 使用卷积神经网络的深度计算机视觉

---

### 练习题
---

###### 1. 与用于图像分类的全连接DNN相比，CNN有什么优势？
> * 因为连接的层仅部分连接并且由于存在大量权重的复用，所以CNN的参数比全连接的DNN要少的多，这使其训练速度快得多，降低了过拟合的风险，并且需要训练数据也少很多。
> * CNN学会了可以检测到特定特征的内核后，便可以在图像中的任何位置检测到该特征。相反，当DNN在一个位置学习某个特征时，他只能在该特定位置检测到它。由于图像通常具有非常重复的特征，因此对于CNN而言，使用较少的训练实例，可能比DNN更好的泛化图像处理任务。
> * 最后，DNN没有像素排列的先验知识。她不知道附近的像素很近。CNN的架构嵌入了此先验知识。较低的层通常在图像的较小区域中标识特征，而较高的层将较低的层的特征组合为较大的特征。这对大多数自然图像都能很好地工作，从而使CNN与DNN相比具有领先的优势。
---
###### 2. 考虑由三个卷积层组成的CNN，每个卷积层有3*3内核，步幅为2和‘same’填充。最低层输出100个特征图，中间层输出200个特征图，最顶层输出400个特征图。输入图像是200*300的RGB图像。CNN的参数总数是多少？如果我们使用32位的浮点数，那么在对单个实例进行预测时，至少该网络需要多少RAM？训练一个包含50个图像的小批量时会怎样？
> 略
---
###### 3. 如果训练CNN时GPU内存不足，可以尝试哪5种方法来解决这个问题？
> 1. 减小小批量的大小。
> 2. 在一层或多层中使用较大的步幅来降低维度。
> 3. 去除一层或多层。
> 4. 使用16位浮点数而不是32位浮点数。
> 5. 在多个设备上分布CNN。
---
###### 4. 为什么要添加最大池化层而不是具有相同步幅的卷积层？
> 最大池化层没有参数，卷积层有很多。
---
###### 5. 你何时要添加局部响应归一化层？
> 局部响应归一化层使最强烈激活的神经元抑制在相同位置但在相邻特征图中的神经元，从而使不同的特征图有针对性相互远离，使他们探索更广泛的特征。它通常用于较低的层，以拥有更大的低层特征池。
---
###### 6. 与LeNet-5相比，你能说出AlexNet的主要创新之处吗？GoogleNet、ResNet、SENet、XCeption的主要创新是什么呢？
> AlexNet的创新在于它更大更深，并且将卷积层直接堆叠在彼此之上，而不是将池化层堆叠在每个卷积层之上。GoogleNet的主要创新在于引入了inception模块，与以前的CNN相比，它具有更少的参数，从而可以有更深的网络。ResNet的主要创新是引入了连接跳跃，这使得超过100层成为可能。可以说，其简单性和一致性也颇有创新性。SENet的主要创新是在inception网络内的每个inception模块或ResNet的每个残差单元之后使用SE块（两层密集深网络）来重新校准特征图中的相对重要性。最后，Xception的主要创新是使用了深度可分离卷积层，他们可以各自识别空间模式和深度模式。
---
###### 7. 什么是全卷积网络？如何将密集层转换为卷积层？
> 完全卷及网络是仅有卷积层和池化层组成的神经网络。FCN可以有效地处理任何宽度和高度的图像。他们对于物体检测和语义分割最有用，因为他们只需查看一次图像（而不必在图像的不同部分多次运行CNN）。如果你有CNN（其上有一些密集层）则可以将这些密集层转换为卷积层来创建FCN：只需用内核大小等于该层输入大小的卷积层代替最低密集层，在密集层中每个神经元使用一个过滤器，并用valid填充。通常，步幅为1，但你可以根据需要将其设置为更高的值。激活函数应与密集层的相同。其它密集层可以用相同的方式转换，但要使用1*1的内核。实际上可以通过适当的重排密集层的权重矩阵这种方式来转换经过训练的CNN。
---
###### 8. 语义分割的主要技术困难是什么？
> 语义分割的主要技术难点是：当信息经过每一层时（尤其是在池化层和步幅大于1的层中），许多空间信息会在CNN中丢失，这个空间信息需要被恢复才能准确地预测出每个像素的类别。
---
