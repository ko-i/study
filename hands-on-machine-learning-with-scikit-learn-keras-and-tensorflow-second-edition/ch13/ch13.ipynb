{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "ch13.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lo0BE1apahk2"
      },
      "source": [
        "# 2020-10-27 created by Akson"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B6Clu3Xahk9"
      },
      "source": [
        "# Code13.1\n",
        "# 玩具数据集\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "X = tf.range(10)\n",
        "print(X)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "print(dataset)\n",
        "\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st9YmBJDahk-"
      },
      "source": [
        "# Code13.2\n",
        "# 链式转换\n",
        "\n",
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset = dataset.repeat(3).batch(7, drop_remainder = True)\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LdC4W7Zahk-"
      },
      "source": [
        "# Code13.3\n",
        "# map\n",
        "\n",
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset = dataset.map(lambda x: x * 2)\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwgVcG7Iahk_"
      },
      "source": [
        "# Code13.4\n",
        "# apply()\n",
        "\n",
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset = dataset.repeat(3).batch(7, drop_remainder = True)\n",
        "\n",
        "# 以下函数在tensorflow2.3.0版本失效\n",
        "# dataset = dataset.apply(tf.data.experimental.unbatch())\n",
        "\n",
        "dataset = dataset.unbatch()\n",
        "\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZtEzva1ahk_"
      },
      "source": [
        "# Code13.5\n",
        "# filter\n",
        "\n",
        "dataset = dataset.map(lambda x: x * 2)\n",
        "dataset = dataset.filter(lambda x: x < 10)\n",
        "\n",
        "for item in dataset.take(5):\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v2SLkO4ahk_"
      },
      "source": [
        "# Code13.6\n",
        "# shuffle\n",
        "\n",
        "dataset = tf.data.Dataset.range(10).repeat(3)\n",
        "\n",
        "for item in dataset:\n",
        "    # print(item)\n",
        "    pass\n",
        "\n",
        "dataset = dataset.shuffle(buffer_size = 5, seed = 42).batch(7)\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOfTWpK_ahlA"
      },
      "source": [
        "# Code13.7\n",
        "# 加载加州住房数据\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 使用api抓取数据\n",
        "housing = fetch_california_housing()\n",
        "# 将数据集拆分成训练集与测试集\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
        "# 将训练集拆分成训练集与验证集\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
        "\n",
        "# 定义归一化对象\n",
        "scaler = StandardScaler()\n",
        "# 归一化拟合训练集的模式\n",
        "scaler.fit(X_train)\n",
        "# 归一化后每个特征的均值\n",
        "X_mean = scaler.mean_\n",
        "# 归一化后每个特征的标准差\n",
        "X_std = scaler.scale_\n",
        "\n",
        "# 将数据存储为文件的函数\n",
        "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
        "    # 获取目录\n",
        "    housing_dir = os.path.join(\"./dataset\", \"house\")\n",
        "    # 如果没有则创建目录\n",
        "    os.makedirs(housing_dir, exist_ok=True)\n",
        "    # 定义文件名格式\n",
        "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
        "    # 文件路径列表\n",
        "    filepaths = []\n",
        "    # 数据长度\n",
        "    m = len(data)\n",
        "    # 间数据拆分成多份分别写入数据文件\n",
        "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
        "        part_csv = path_format.format(name_prefix, file_idx)\n",
        "        filepaths.append(part_csv)\n",
        "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
        "            if header is not None:\n",
        "                f.write(header)\n",
        "                f.write(\"\\n\")\n",
        "            for row_idx in row_indices:\n",
        "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
        "                f.write(\"\\n\")\n",
        "    return filepaths\n",
        "\n",
        "# 数据变成1维\n",
        "train_data = np.c_[X_train, y_train]\n",
        "valid_data = np.c_[X_valid, y_valid]\n",
        "test_data = np.c_[X_test, y_test]\n",
        "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
        "header = \",\".join(header_cols)\n",
        "\n",
        "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
        "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
        "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-7YlAP2ahlA"
      },
      "source": [
        "# Code13.8\n",
        "# 交织数据\n",
        "\n",
        "print(train_filepaths)\n",
        "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed = 42)\n",
        "\n",
        "n_readers = 5\n",
        "dataset = filepath_dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length = n_readers)\n",
        "\n",
        "for line in dataset.take(5):\n",
        "    print(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLKg_vQTahlB"
      },
      "source": [
        "# Code13.9\n",
        "# 一些预处理\n",
        "\n",
        "n_inputs = 8\n",
        "\n",
        "def preprocess(line):\n",
        "    # 定义默认格式，首先是8个浮点数零作为缺省值，最后一列作为标签的缺省值，是一个32位浮点数，但没有赋值\n",
        "    defs = [0.] * n_inputs + [tf.constant([], dtype = tf.float32)]\n",
        "    fields = tf.io.decode_csv(line, record_defaults = defs)\n",
        "    # stack()将张量堆叠为一维数组\n",
        "    x = tf.stack(fields[:-1])\n",
        "    y = tf.stack(fields[-1:])\n",
        "    \n",
        "    # 返回处理过的元组\n",
        "    return (x - X_mean) / X_std, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN1GYm14ahlB"
      },
      "source": [
        "# Code13.10\n",
        "# 整合以上步骤\n",
        "\n",
        "def csv_reader_dataset(filepaths, repeat = 1, n_readers = 5, n_read_threads = None, shuffle_buffer_size = 10000, n_parse_threads = 5, batch_size = 32):\n",
        "    # 首先从文件路径列表中获取到所有文件\n",
        "    dataset = tf.data.Dataset.list_files(filepaths)\n",
        "    # 然后交织读取文件列表里每个文件中的每一行（跳过标题）\n",
        "    dataset = dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length = n_readers, num_parallel_calls = n_read_threads)\n",
        "    # 对数据集中的每一行内容做预处理（将本来字节的数据格式变成张量并做标准化处理）\n",
        "    dataset = dataset.map(preprocess, num_parallel_calls = n_parse_threads)\n",
        "    # 对数据集中的内容做乱序处理\n",
        "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
        "    \n",
        "    return dataset.batch(batch_size).prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rihkPHuUahlC"
      },
      "source": [
        "# Code13.11\n",
        "# 获取数据集\n",
        "\n",
        "train_set = csv_reader_dataset(train_filepaths, repeat = None)\n",
        "valid_set = csv_reader_dataset(valid_filepaths)\n",
        "test_set = csv_reader_dataset(test_filepaths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1Ni8KGOahlC"
      },
      "source": [
        "# Code13.12\n",
        "# training\n",
        "\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation = 'relu', input_shape = X_train.shape[1:]),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "optimizer = keras.optimizers.SGD(lr = 1e-3)\n",
        "model.compile(loss = 'mae', optimizer = optimizer)\n",
        "\n",
        "batch_size = 32\n",
        "model.fit(train_set, steps_per_epoch = len(X_train) // batch_size, epochs = 10, validation_data = valid_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SgTW5N8ahlC"
      },
      "source": [
        "# Code13.13\n",
        "# TFRecord\n",
        "\n",
        "with tf.io.TFRecordWriter('my_data.tfrecord') as f:\n",
        "    f.write(b'This is my first record')\n",
        "    f.write(b'And this is my second record')\n",
        "\n",
        "filepaths = ['my_data.tfrecord']\n",
        "dataset = tf.data.TFRecordDataset(filepaths)\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNWvwx1OahlD"
      },
      "source": [
        "# Code13.14\n",
        "# 压缩的\n",
        "\n",
        "options = tf.io.TFRecordOptions(compression_type = 'GZIP')\n",
        "with tf.io.TFRecordWriter('my_compressed.tfrecord', options) as f:\n",
        "    f.write(b'This is my first record')\n",
        "    f.write(b'And this is my second record')\n",
        "    \n",
        "dataset = tf.data.TFRecordDataset(['my_compressed.tfrecord'], compression_type = 'GZIP')\n",
        "\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xohzRfoMahlD"
      },
      "source": [
        "# Code13.15\n",
        "\n",
        "BytesList = tf.train.BytesList\n",
        "FloatList = tf.train.FloatList\n",
        "Int64List = tf.train.Int64List\n",
        "Feature = tf.train.Feature\n",
        "Features = tf.train.Features\n",
        "Example = tf.train.Example\n",
        "\n",
        "person_example = Example(\n",
        "    features = Features(\n",
        "        feature = {\n",
        "            'name': Feature(bytes_list = BytesList(value = [b'Alice'])),\n",
        "            'id': Feature(int64_list = Int64List(value = [123])),\n",
        "            'emails': Feature(bytes_list = BytesList(value = [b'a@b.com', b'c@d.com']))\n",
        "        }))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNcH3yYRahlD"
      },
      "source": [
        "# Code13.16\n",
        "# 对Example对象进行序列化并保存到文件中\n",
        "\n",
        "with tf.io.TFRecordWriter('my_contacts.tfrecord') as f:\n",
        "    f.write(person_example.SerializeToString())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i19_0xHzahlD"
      },
      "source": [
        "# Code13.17\n",
        "# 从文件中加载\n",
        "\n",
        "# 定义描述字典\n",
        "feature_description = {\n",
        "    'name': tf.io.FixedLenFeature([], tf.string, default_value = ''),\n",
        "    'id': tf.io.FixedLenFeature([], tf.int64, default_value = 0),\n",
        "    'emails': tf.io.VarLenFeature(tf.string)\n",
        "}\n",
        "\n",
        "for serialized_example in tf.data.TFRecordDataset(['my_contacts.tfrecord']):\n",
        "    parsed_example = tf.io.parse_single_example(serialized_example, feature_description)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJXcSALFahlE"
      },
      "source": [
        "# Code13.18\n",
        "# 也可以整批加载\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(['my_contacts.tfrecord']).batch(10)\n",
        "for serialized_examples in dataset:\n",
        "    parsed_examples = tf.io.parse_example(serialized_examples, feature_description)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEjRKC3mahlE"
      },
      "source": [
        "# Code13.19\n",
        "# 使用tensorflow_datasets\n",
        "\n",
        "# 需要在当前python环境下输入以下命令来安装\n",
        "# pip install tensorflow_datasets\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "dataset = tfds.load(name = 'mnist', batch_size = 32, as_supervised = True)\n",
        "mnist_train = dataset['train'].repeat().prefetch(1)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape = [28, 28, 1]),\n",
        "    keras.layers.Lambda(lambda images: tf.cast(images, tf.float32)),\n",
        "    keras.layers.Dense(10, activation = 'softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = keras.optimizers.SGD(lr = 1e-3), metrics = ['accuracy'])\n",
        "model.fit(mnist_train, steps_per_epoch = 60000 // 32, epochs = 5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}