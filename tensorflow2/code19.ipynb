{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN9XVIZD1lZPvfdlOaEV0Zu"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrn9HbZItuyT"
      },
      "source": [
        "import io\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpeQ1wBUyudV"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DnMZsx8yxpF"
      },
      "source": [
        "SEED = 42\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6edZe8F7y2DQ"
      },
      "source": [
        "sentence = \"The wide road shimmered in the hot sun\"\n",
        "tokens = list(sentence.lower().split())\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwYj7oPSzEAK"
      },
      "source": [
        "len(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_zqSkdFzIH7"
      },
      "source": [
        "vocab, index = {}, 1\n",
        "vocab['<pad>'] = 0\n",
        "\n",
        "for token in tokens:\n",
        "    if token not in vocab:\n",
        "        vocab[token] = index\n",
        "        index += 1\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GnTKFnTzgDl"
      },
      "source": [
        "inverse_vocab = {index: token for token, index in vocab.items()}\n",
        "print(inverse_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffYPh9Piz4E4"
      },
      "source": [
        "example_sequence = [vocab[word] for word in tokens]\n",
        "example_sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHr_krFZ0GhV"
      },
      "source": [
        "window_size = 2\n",
        "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(example_sequence, vocab_size, window_size, 0)\n",
        "print(len(positive_skip_grams))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7dIpEVB2dfb"
      },
      "source": [
        "positive_skip_grams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y7ni9nN2fLj"
      },
      "source": [
        "for target, contex in positive_skip_grams[:]:\n",
        "    print(\"({}, {}): ({}, {})\".format(target, contex, inverse_vocab[target], inverse_vocab[contex]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPwLUTn-3RAL"
      },
      "source": [
        "target_word, context_word = positive_skip_grams[0]\n",
        "\n",
        "num_ns = 4\n",
        "\n",
        "context_class = tf.reshape(tf.constant(context_word, dtype='int64'), (1, 1))\n",
        "context_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w613Ffxy9QIa"
      },
      "source": [
        "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "    true_classes=context_class,\n",
        "    num_true=1,\n",
        "    num_sampled=num_ns,\n",
        "    unique=True,\n",
        "    range_max=vocab_size,\n",
        "    seed=SEED,\n",
        "    name='negative_sampling'\n",
        ")\n",
        "\n",
        "print(negative_sampling_candidates)\n",
        "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM85vUKr144p"
      },
      "source": [
        "negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
        "negative_sampling_candidates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzXeDKCV5t9L"
      },
      "source": [
        "context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G04NuY9a5-BJ"
      },
      "source": [
        "label = tf.constant([1] + [0]*num_ns, dtype='int64')\n",
        "label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIXYlHy862AE"
      },
      "source": [
        "target_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTABvxwD6uFK"
      },
      "source": [
        "target = tf.squeeze(target_word)\n",
        "target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toB1BsQb642u"
      },
      "source": [
        "context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6aK02oK68KC"
      },
      "source": [
        "tf.squeeze(context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1i6EvD56_3p"
      },
      "source": [
        "context = tf.squeeze(context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9opOMP37Cr6"
      },
      "source": [
        "label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jqJBKXm7EX3"
      },
      "source": [
        "label = tf.squeeze(label)\n",
        "label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrBB8pN67IVO"
      },
      "source": [
        "print(f\"target_index    : {target}\")\n",
        "print(f\"target_word     : {inverse_vocab[target_word]}\")\n",
        "print(f\"context_indices : {context}\")\n",
        "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n",
        "print(f\"label           : {label}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQiiklm47yba"
      },
      "source": [
        "print(\"target: {}\".format(target))\n",
        "print(\"context: {}\".format(context))\n",
        "print(\"label: {}\".format(label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtxYBDqu8SgY"
      },
      "source": [
        "tf.keras.preprocessing.sequence.make_sampling_table(size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHHWfbYb-J_Y"
      },
      "source": [
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "    targets, contexts, labels = [], [], []\n",
        "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "    for sequence in tqdm.tqdm(sequences):\n",
        "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "            sequence,\n",
        "            vocab_size,\n",
        "            window_size,\n",
        "            0,\n",
        "            sampling_table=sampling_table\n",
        "        )\n",
        "\n",
        "        for target_word, context_word in positive_skip_grams:\n",
        "            context_class = tf.expand_dims(tf.constant([context_word], dtype='int64'), 1)\n",
        "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "                true_classes=context_class,\n",
        "                num_true=1,\n",
        "                num_sampled=num_ns,\n",
        "                unique=True,\n",
        "                range_max=vocab_size,\n",
        "                seed=SEED,\n",
        "                name=\"negative_sampling\"\n",
        "            )\n",
        "\n",
        "            negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
        "            context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "            label = tf.constant([1] + [0]*num_ns, dtype='int64')\n",
        "\n",
        "            targets.append(target_word)\n",
        "            contexts.append(context)\n",
        "            labels.append(label)\n",
        "        \n",
        "    return targets, contexts, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtVyv3T8D6lC"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6llAut4VEbPp"
      },
      "source": [
        "with open(path_to_file) as f:\n",
        "    lines = f.read().splitlines()\n",
        "for line in lines[:20]:\n",
        "    print(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plBZIc2wFuB6"
      },
      "source": [
        "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWEbMehCGopw"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    return tf.strings.regex_replace(lowercase, '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "vocab_size = 4096\n",
        "sequence_length = 10\n",
        "\n",
        "vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_sequence_length= sequence_length,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5njgxu6UIecz"
      },
      "source": [
        "vectorize_layer.adapt(text_ds.batch(1024))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHXA7tCXIrXs"
      },
      "source": [
        "inverse_vocab = vectorize_layer.get_vocabulary()\n",
        "print(inverse_vocab[:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-C7sgRUI6Au"
      },
      "source": [
        "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9KphZa4JPIF"
      },
      "source": [
        "sequences = list(text_vector_ds.as_numpy_iterator())\n",
        "print(len(sequences))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "potfs6ckJxT8"
      },
      "source": [
        "for seq in sequences[:5]:\n",
        "    print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9ZqSd2vK3RZ"
      },
      "source": [
        "targets, contexts, labels = generate_training_data(\n",
        "    sequences,\n",
        "    2,\n",
        "    4,\n",
        "    vocab_size,\n",
        "    SEED\n",
        ")\n",
        "\n",
        "print(len(targets), len(contexts), len(labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKyiuNcxL-65"
      },
      "source": [
        "print(targets[:5])\n",
        "print(contexts[:5])\n",
        "print(labels[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdIbcgFLMpPK"
      },
      "source": [
        "BATCH_SIZE=1024\n",
        "BUFFER_SIZE=10000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Zms-24WNm3U"
      },
      "source": [
        "dataset = dataset.cache().prefetch(AUTOTUNE)\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4vOPtgGNxat"
      },
      "source": [
        "class Word2Vec(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(Word2Vec, self).__init__()\n",
        "\n",
        "        self.target_embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=1, name=\"w2v_embedding\")\n",
        "        self.context_embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=num_ns+1)\n",
        "        self.dots = tf.keras.layers.Dot(axes=(3,2))\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "    \n",
        "    def call(self, pair):\n",
        "        target, context = pair\n",
        "        word_emb = self.target_embedding(target)\n",
        "        context_emb = self.context_embedding(context)\n",
        "        dots = self.dots([context_emb, word_emb])\n",
        "        return self.flatten(dots)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDA-lbtdRPlP"
      },
      "source": [
        "def custom_loss(x_logit, y_true):\n",
        "    return tf.nn.sigmoid_cross_entropy_with_logits(y_true, x_logit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8byWARyStEv"
      },
      "source": [
        "embedding_dim = 128\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile('adam', loss=tf.keras.losses.CategoricalCrossentropy(True), metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh5C87wTTDYY"
      },
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='logs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrzEBMitTVJI"
      },
      "source": [
        "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R3F4tuETg77"
      },
      "source": [
        "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8TVkQQGU9e2"
      },
      "source": [
        "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for index, word in enumerate(vocab):\n",
        "    if index==0:\n",
        "        continue\n",
        "    \n",
        "    vec = weights[index]\n",
        "    out_v.write('\\t'.join([str(x) for x in vec]) + '\\n')\n",
        "    out_m.write(word+'\\n')\n",
        "\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsNuQ98aWEYS"
      },
      "source": [
        "try:\n",
        "    from google.colab import files\n",
        "    files.download('vectors.tsv')\n",
        "    files.download('metadata.tsv')\n",
        "except Exception:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}