{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMki8Omh90yinI/928egetF"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHw0wH2xODwQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZcZ0aA0OjvQ"
      },
      "source": [
        "def get_model():\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Dense(1, input_dim=784))\n",
        "    model.compile(optimizer=keras.optimizers.RMSprop(0.1), loss='mean_squared_error', metrics='mean_absolute_error')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSqpAZW3PSgG"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
        "\n",
        "x_train = x_train[:1000]\n",
        "y_train = y_train[:1000]\n",
        "x_test = x_test[:1000]\n",
        "y_test = y_test[:1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9R1CGpIPzAH"
      },
      "source": [
        "class CustomCallback(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print('Starting training; got log keys: {}'.format(keys))\n",
        "    \n",
        "    def on_train_end(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print('Stoping training; got log keys: {}'.format(keys))\n",
        "    \n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print('Start epoch {}, got log keys: {}'.format(epoch, keys))\n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print('End epoch {}, got log keys: {}'.format(epoch, keys))\n",
        "    \n",
        "    def on_test_begin(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print('Starting testing; got log keys: {}'.format(keys))\n",
        "    \n",
        "    def on_test_end(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print('Stoping testing; got log keys: {}'.format(keys))\n",
        "    \n",
        "    def on_predict_begin(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print('Starting predicting; got log keys: {}'.format(keys))\n",
        "    \n",
        "    def on_predict_end(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print('Stoping predicting; got log keys: {}'.format(keys))\n",
        "    \n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print('...Training: start of batch {}; got log keys: {}'.format(batch, keys))\n",
        "    \n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print('...Training: end of batch {}; got log keys: {}'.format(batch, keys))\n",
        "    \n",
        "    def on_test_batch_begin(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print('...Testing: start of batch {}; got log keys: {}'.format(batch, keys))\n",
        "    \n",
        "    def on_test_batch_end(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print('...Testing: end of batch {} got log keys: {}'.format(batch, keys))\n",
        "    \n",
        "    def on_predict_batch_begin(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print('...Predicting: start of batch {}; got log keys: {}'.format(batch, keys))\n",
        "\n",
        "    def on_predict_batch_end(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print('...Predicting: end of batch {}; got log keys: {}'.format(batch, keys))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wRth92hWVD7"
      },
      "source": [
        "model = get_model()\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=128, epochs=1, verbose=0, validation_split=0.5, callbacks=[CustomCallback()])\n",
        "\n",
        "res = model.evaluate(x_test, y_test, batch_size=128, verbose=0, callbacks=[CustomCallback()])\n",
        "\n",
        "res = model.predict(x_test, batch_size=128, verbose=0, callbacks=[CustomCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOesIpsBXr_a"
      },
      "source": [
        "class LossAndErrorPrintingCallback(keras.callbacks.Callback):\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        print('For batch {}, loss is {:7.2f}.'.format(batch, logs['loss']))\n",
        "    \n",
        "    def on_test_batch_end(self, batch, logs=None):\n",
        "        print('For batch {}, loss is {:7.2f}.'.format(batch, logs['loss']))\n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print('The average loss for epoch {} is {:7.2f} and mean absolute error is {:7.2f}'.format(epoch, logs['loss'], logs['mean_absolute_error']))\n",
        "\n",
        "model = get_model()\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=128, verbose=0, epochs=2, callbacks=[LossAndErrorPrintingCallback()])\n",
        "\n",
        "res = model.evaluate(x_test, y_test, batch_size=128, verbose=0, callbacks=[LossAndErrorPrintingCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGhUVdj1b72X"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class EarlyStopingAtMinLoss(keras.callbacks.Callback):\n",
        "    def __init__(self, patience=0):\n",
        "        super(EarlyStopingAtMinLoss, self).__init__()\n",
        "        self.patience = patience\n",
        "        self.best_weights = None\n",
        "    \n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.wait = 0\n",
        "        self.stopped_epoch = 0\n",
        "        self.best = np.Inf\n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        current = logs.get('loss')\n",
        "        if np.less(current, self.best):\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "            self.best_weights = self.model.get_weights()\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                self.model.stop_training = True\n",
        "                print('Restoring model weights from the end of the best epoch')\n",
        "                self.model.set_weights(self.best_weights)\n",
        "    \n",
        "    def on_train_end(self, logs=None):\n",
        "        if self.stopped_epoch > 0:\n",
        "            print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))\n",
        "\n",
        "model = get_model()\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=64, steps_per_epoch=5, epochs=30, verbose=0, callbacks=[LossAndErrorPrintingCallback(), EarlyStopingAtMinLoss()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7GH6YALicew"
      },
      "source": [
        "class CustomLearningRateScheduler(keras.callbacks.Callback):\n",
        "    def __init__(self, schedule):\n",
        "        super(CustomLearningRateScheduler, self).__init__()\n",
        "        self.schedule = schedule\n",
        "    \n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        if not hasattr(self.model.optimizer, 'lr'):\n",
        "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
        "        \n",
        "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
        "        scheduled_lr = self.schedule(epoch, lr)\n",
        "\n",
        "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
        "        print('\\nEpoch %05d: Learning rate is %6.4f' % (epoch, scheduled_lr))\n",
        "\n",
        "LR_SCHEDULE = [\n",
        "    (3, 0.05),\n",
        "    (6, 0.01),\n",
        "    (9, 0.005),\n",
        "    (12, 0.001)\n",
        "]\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "    if epoch < LR_SCHEDULE[0][0] or epoch > LR_SCHEDULE[-1][0]:\n",
        "        return lr\n",
        "    \n",
        "    for i in range(len(LR_SCHEDULE)):\n",
        "        if epoch == LR_SCHEDULE[i][1]:\n",
        "            return LR_SCHEDULE[i][1]\n",
        "        \n",
        "    return lr\n",
        "\n",
        "\n",
        "model = get_model()\n",
        "model.fit(x_train, y_train, batch_size=64, steps_per_epoch=5, epochs=15, verbose=0, callbacks=[LossAndErrorPrintingCallback(), CustomLearningRateScheduler(lr_schedule)])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}