{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMHMZzWf+KF18IXPPQ4EFm4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ2QwMF6nTE3"
      },
      "source": [
        "!pip install -q tensorflow-text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-LWHiyOnrGU"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpQmDWBznxsK"
      },
      "source": [
        "docs = tf.constant([u'Everything not saved will be lost.'.encode('UTF-16-BE'), u'Sad☹'.encode('UTF-16-BE')])\n",
        "utf8_docs = tf.strings.unicode_transcode(docs, input_encoding='UTF-16-BE', output_encoding='UTF-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcwESjzDoylL"
      },
      "source": [
        "tokenizer = text.WhitespaceTokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYSmqTv6p1sm"
      },
      "source": [
        "tokens = tokenizer.tokenize(['Everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWhEBcN5qDDc"
      },
      "source": [
        "print(tokens.to_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQsmt4IIqKAF"
      },
      "source": [
        "tokenizer = text.UnicodeScriptTokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NucCuqEproxt"
      },
      "source": [
        "tokens = tokenizer.tokenize(['Everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmdWiSU4oOnM"
      },
      "source": [
        "print(tokens.to_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VShQKWAGrxap"
      },
      "source": [
        "tokens = tf.strings.unicode_split([u'今年以前'.encode('UTF-8')], 'UTF-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHYywgodsGuB"
      },
      "source": [
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieVbv6wLsHo9"
      },
      "source": [
        "tokenizer = text.UnicodeScriptTokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX8HEexOs9y5"
      },
      "source": [
        "(tokens, start_offsets, end_offsets) = tokenizer.tokenize_with_offsets(['Everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRyDtAPYtJQT"
      },
      "source": [
        "print(tokens)\n",
        "print(start_offsets)\n",
        "print(end_offsets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdoFGx5itNmh"
      },
      "source": [
        "docs = tf.data.Dataset.from_tensor_slices([['Never tell me the odds.'], [\"It's a trap!\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mSo43QQuNPz"
      },
      "source": [
        "docs.as_numpy_iterator().next()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBPG8--NuUWd"
      },
      "source": [
        "tokenizer = text.WhitespaceTokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3sPK0RuuZ7U"
      },
      "source": [
        "tokenized_docs = docs.map(lambda x: tokenizer.tokenize(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9mDlY4Tump_"
      },
      "source": [
        "tokenized_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZEO44TduoFt"
      },
      "source": [
        "iterator = iter(tokenized_docs)\n",
        "\n",
        "print(next(iterator).to_list())\n",
        "print(next(iterator).to_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXC1whqsutvm"
      },
      "source": [
        "tokenizer = text.WhitespaceTokenizer()\n",
        "tokens = tokenizer.tokenize(['Everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQBE7N48wohv"
      },
      "source": [
        "f1 = text.wordshape(tokens, text.WordShape.HAS_TITLE_CASE)\n",
        "f2 = text.wordshape(tokens, text.WordShape.IS_UPPERCASE)\n",
        "f3 = text.wordshape(tokens, text.WordShape.HAS_SOME_PUNCT_OR_SYMBOL)\n",
        "f4 = text.wordshape(tokens, text.WordShape.IS_NUMERIC_VALUE)\n",
        "\n",
        "print(f1)\n",
        "print(f2)\n",
        "print(f3)\n",
        "print(f4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiRJD1ZfxOJf"
      },
      "source": [
        "tokenizer = text.WhitespaceTokenizer()\n",
        "tokens = tokenizer.tokenize(['Everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDYaFImvyqJh"
      },
      "source": [
        "bigrams = text.ngrams(tokens, 2, reduction_type=text.Reduction.STRING_JOIN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOYqUQOsy3LX"
      },
      "source": [
        "print(bigrams)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}