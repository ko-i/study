{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPcqJIotXyFj2N42TvUisLL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ko-i/study/blob/main/tensorflow2/code06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zeUetOzgnLN"
      },
      "source": [
        "!pip install -q -U tensorflow-text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KogJl6Pfdm1"
      },
      "source": [
        "import collections\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as tf_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "922HmqF0geZp"
      },
      "source": [
        "data_url = 'https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz'\n",
        "\n",
        "dataset = tf.keras.utils.get_file('stack_overflow_16k.tar.gz', data_url, untar = True, cache_dir = 'stack_overflow', cache_subdir = '')\n",
        "\n",
        "dataset_dir = pathlib.Path(dataset).parent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kmvz3bNHhwaA"
      },
      "source": [
        "dataset_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqX8jtJch4Ah"
      },
      "source": [
        "list(dataset_dir.iterdir())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eCdwpnRiAbM"
      },
      "source": [
        "train_dir = dataset_dir/'train'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWoJjN2aiKtx"
      },
      "source": [
        "list(train_dir.iterdir())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxQLXzomiN1d"
      },
      "source": [
        "sample_file = train_dir/'python/1755.txt'\n",
        "\n",
        "with open(sample_file) as f:\n",
        "    print(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Efv7ulBWijmi"
      },
      "source": [
        "batch_size = 32\n",
        "seed = 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1nbLaH2pf8O"
      },
      "source": [
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(train_dir, batch_size = batch_size, validation_split = 0.2, subset = 'training', seed = seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLKqZknOqxK0"
      },
      "source": [
        "raw_train_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAhzx55fqDjD"
      },
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "    for i in range(10):\n",
        "        print(\"Question: \", text_batch.numpy()[i])\n",
        "        print(\"Label: \", label_batch.numpy()[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhnwCb9NsafH"
      },
      "source": [
        "for i, label in enumerate(raw_train_ds.class_names):\n",
        "    print(\"Label \", i, \" is corresponds to \", label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-raX-W0Ssz96"
      },
      "source": [
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(train_dir, batch_size = batch_size, validation_split = 0.2, subset = 'validation', seed = seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUpY9Ey1tbEb"
      },
      "source": [
        "test_dir = dataset_dir/'test'\n",
        "\n",
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(test_dir, batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lNDumCctq63"
      },
      "source": [
        "VOCAB_SIZE = 10000\n",
        "\n",
        "binary_vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens = VOCAB_SIZE, output_mode = 'binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ai9RURKvw7L"
      },
      "source": [
        "MAX_SEQUENCE_LENGTH= 250\n",
        "\n",
        "int_vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens = VOCAB_SIZE, output_mode = 'int', output_sequence_length = MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6mA7cGBwPQg"
      },
      "source": [
        "train_text = raw_train_ds.map(lambda text, labels: text)\n",
        "\n",
        "train_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE7XAXeWw2iB"
      },
      "source": [
        "for item in train_text.take(1):\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78JOY0Bkw9H0"
      },
      "source": [
        "binary_vectorize_layer.adapt(train_text)\n",
        "int_vectorize_layer.adapt(train_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14802nEqxPPS"
      },
      "source": [
        "def binary_vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "\n",
        "    return binary_vectorize_layer(text), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLOOgQjLysgv"
      },
      "source": [
        "def int_vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "\n",
        "    return int_vectorize_layer(text), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZaqCznGy78e"
      },
      "source": [
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "\n",
        "first_question, first_label = text_batch[0], label_batch[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PoeyS3kzecp"
      },
      "source": [
        "print(first_label)\n",
        "print(first_question)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXVCy-pRzkq6"
      },
      "source": [
        "print(\"'binary' vectorized question:\", binary_vectorize_text(first_question, first_label)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJISOgGX0yRz"
      },
      "source": [
        "print(\"'int' vectorized question:\", int_vectorize_text(first_question, first_label)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpprYaUk09AO"
      },
      "source": [
        "print(\"1289 ----> \", int_vectorize_layer.get_vocabulary()[1289])\n",
        "print(\"313 ----> \", int_vectorize_layer.get_vocabulary()[313])\n",
        "print(\"10000 ----> \", int_vectorize_layer.get_vocabulary()[9999])\n",
        "print(\"Vocabulary size: {}\".format(len(int_vectorize_layer.get_vocabulary())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLm79FN119h2"
      },
      "source": [
        "binary_train_ds = raw_train_ds.map(binary_vectorize_text)\n",
        "binary_val_ds = raw_val_ds.map(binary_vectorize_text)\n",
        "binary_test_ds = raw_test_ds.map(binary_vectorize_text)\n",
        "\n",
        "int_train_ds = raw_train_ds.map(int_vectorize_text)\n",
        "int_val_ds = raw_val_ds.map(int_vectorize_text)\n",
        "int_test_ds = raw_test_ds.map(int_vectorize_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-wUVkJ84Gyt"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def configure_dataset(dataset):\n",
        "    return dataset.cache().prefetch(buffer_size = AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnz58jUv40Ca"
      },
      "source": [
        "binary_train_ds = configure_dataset(binary_train_ds)\n",
        "binary_val_ds = configure_dataset(binary_val_ds)\n",
        "binary_test_ds = configure_dataset(binary_test_ds)\n",
        "\n",
        "int_train_ds = configure_dataset(int_train_ds)\n",
        "int_val_ds = configure_dataset(int_val_ds)\n",
        "int_test_ds = configure_dataset(int_test_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZiJZmGq5flu"
      },
      "source": [
        "binary_model = tf.keras.Sequential([tf.keras.layers.Dense(4)])\n",
        "\n",
        "binary_model.compile(optimizer = 'adam', loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), metrics = ['accuracy'])\n",
        "\n",
        "history = binary_model.fit(binary_train_ds, validation_data = binary_val_ds, epochs = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMpX-wsV7QcZ"
      },
      "source": [
        "def create_model(vocab_size, num_labels):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, 64, mask_zero = True),\n",
        "        tf.keras.layers.Conv1D(64, 5, 2, 'valid', activation = 'relu'),\n",
        "        tf.keras.layers.GlobalMaxPooling1D(),\n",
        "        tf.keras.layers.Dense(num_labels),\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VyPY1BY-63x"
      },
      "source": [
        "int_model = create_model(VOCAB_SIZE + 1, 4)\n",
        "\n",
        "int_model.compile(optimizer = 'adam', loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), metrics = ['accuracy'])\n",
        "\n",
        "history = int_model.fit(int_train_ds, validation_data = int_val_ds, epochs = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wCPuwoo_gpv"
      },
      "source": [
        "print(binary_model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZb9x2hW_9Zp"
      },
      "source": [
        "print(int_model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6RZ6YMfABbo"
      },
      "source": [
        "binary_loss, binary_acc = binary_model.evaluate(binary_test_ds)\n",
        "int_loss, int_acc = int_model.evaluate(int_test_ds)\n",
        "\n",
        "print(\"Binary model acc: {:2.2%}\".format(binary_acc))\n",
        "print(\"Int model acc: {:2.2%}\".format(int_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuMgNnZNA-oF"
      },
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "    binary_vectorize_layer,\n",
        "    binary_model,\n",
        "    tf.keras.layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "export_model.compile(optimizer = 'adam', loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), metrics = ['accuracy'])\n",
        "\n",
        "loss, acc = export_model.evaluate(raw_test_ds)\n",
        "print(\"Acc: {:2.2%}\".format(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXm9HhdAC9J0"
      },
      "source": [
        "def get_string_labels(predicted_score_batch):\n",
        "    predicted_int_labels = tf.argmax(predicted_score_batch, axis = 1)\n",
        "    predicted_labels = tf.gather(raw_train_ds.class_names, predicted_int_labels)\n",
        "\n",
        "    return predicted_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6iYslna08P8"
      },
      "source": [
        "inputs = [\n",
        "    \"how do I extract keys from a dict into a list?\",  # python\n",
        "    \"debug public static void main(string[] args) {...}\",  # java\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hFd-4Gq1O93"
      },
      "source": [
        "predicted_scores = export_model.predict(inputs)\n",
        "predicted_labels = get_string_labels(predicted_scores)\n",
        "\n",
        "for input, label in zip(inputs, predicted_labels):\n",
        "    print(\"Question: \", input)\n",
        "    print(\"Label: \", label.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6SmULqh1t4y"
      },
      "source": [
        "DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
        "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
        "\n",
        "for name in FILE_NAMES:\n",
        "    text_dir = tf.keras.utils.get_file(name, origin = DIRECTORY_URL + name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdOhRlFn7ikW"
      },
      "source": [
        "parent_dir = pathlib.Path(text_dir).parent\n",
        "\n",
        "list(parent_dir.iterdir())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y1SMAWn7ynX"
      },
      "source": [
        "def labeler(example, index):\n",
        "    return example, tf.cast(index, tf.int64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaYl8Qfp_GJj"
      },
      "source": [
        "labeled_data_sets = []\n",
        "\n",
        "for i, file_name in enumerate(FILE_NAMES):\n",
        "    lines_datasets = tf.data.TextLineDataset(str(parent_dir/file_name))\n",
        "    labeled_dataset = lines_datasets.map(lambda ex: labeler(ex, i))\n",
        "\n",
        "    labeled_data_sets.append(labeled_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzc4LYdf_tLo"
      },
      "source": [
        "BUFFER_SIZE = 50000\n",
        "BATCH_SIZE = 64\n",
        "VALIDATION_SIZE = 5000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kevGASgI_6Cd"
      },
      "source": [
        "labeled_data_sets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwVQqtjo_9g0"
      },
      "source": [
        "all_labeled_data = labeled_data_sets[0]\n",
        "\n",
        "for labeled_dataset in labeled_data_sets[1:]:\n",
        "    all_labeled_data = all_labeled_data.concatenate(labeled_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxVY9b_cAwgD"
      },
      "source": [
        "all_labeled_data = all_labeled_data.shuffle(BUFFER_SIZE, reshuffle_each_iteration = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QerX8jzJBUPE"
      },
      "source": [
        "for text, label in all_labeled_data.take(10):\n",
        "    print(\"Sentence: \", text.numpy())\n",
        "    print(\"Label: \", label.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVIZnM1oBaMx"
      },
      "source": [
        "tokenizer = tf_text.UnicodeScriptTokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBeODf2nCRUg"
      },
      "source": [
        "def tokenize(text, unused_label):\n",
        "    lower_case = tf_text.case_fold_utf8(text)\n",
        "\n",
        "    return tokenizer.tokenize(lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B23ZEdnBC0OJ"
      },
      "source": [
        "tokenized_ds = all_labeled_data.map(tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzZAglKe6Hk8"
      },
      "source": [
        "for text_batch in tokenized_ds.take(5):\n",
        "  print(\"Tokens: \", text_batch.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2kFZdpt25TM"
      },
      "source": [
        "tokenized_ds = configure_dataset(tokenized_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp5fZzGW5IZm"
      },
      "source": [
        "vocab_dict = collections.defaultdict(lambda: 0)\n",
        "\n",
        "for toks in tokenized_ds.as_numpy_iterator():\n",
        "    for tok in toks:\n",
        "        vocab_dict[tok] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ3E4f105pvU"
      },
      "source": [
        "vocab = sorted(vocab_dict.items(), key = lambda x: x[1], reverse = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8IPHq0I3bEj"
      },
      "source": [
        "vocab = [token for token, count in vocab]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLkUTCCC26gX"
      },
      "source": [
        "vocab = vocab[:VOCAB_SIZE]\n",
        "vocab_size = len(vocab);\n",
        "print(\"vocab size: \", vocab_size)\n",
        "print(\"first five vocab: \", vocab[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pzv4cORs8N8l"
      },
      "source": [
        "keys = vocab\n",
        "values = range(2, len(vocab) + 2)\n",
        "\n",
        "init = tf.lookup.KeyValueTensorInitializer(keys, values, key_dtype=tf.string, value_dtype=tf.int64)\n",
        "num_oov_buckets = 1\n",
        "\n",
        "vocab_table = tf.lookup.StaticVocabularyTable(init, num_oov_buckets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5Nok6m9-d_i"
      },
      "source": [
        "def preprocess_text(text, label):\n",
        "    standardized = tf_text.case_fold_utf8(text)\n",
        "    tokenized = tokenizer.tokenize(standardized)\n",
        "    vectorized = vocab_table.lookup(tokenized)\n",
        "\n",
        "    return vectorized, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te3bQX4f_FnO"
      },
      "source": [
        "example_text, example_label = next(iter(all_labeled_data))\n",
        "print(\"Sentence: \", example_text)\n",
        "vectorized_text, example_label = preprocess_text(example_text, example_label)\n",
        "print(\"Vectorizer sentence: \", vectorized_text.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM1_iYM__hZ0"
      },
      "source": [
        "all_encoded_data = all_labeled_data.map(preprocess_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVCW_bGq_-zr"
      },
      "source": [
        "train_data = all_encoded_data.skip(VALIDATION_SIZE).shuffle(BUFFER_SIZE)\n",
        "validation_data = all_encoded_data.take(VALIDATION_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5N0JWuIBZ2z"
      },
      "source": [
        "train_data = train_data.padded_batch(BATCH_SIZE)\n",
        "validation_data = validation_data.padded_batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V62U_57BB0hf"
      },
      "source": [
        "sample_text, sample_label = next(iter(validation_data))\n",
        "print(\"Text batch shape: \", sample_text.shape)\n",
        "print(\"Label batch shape: \", sample_label.shape)\n",
        "print(\"First text example: \", sample_text[0])\n",
        "print(\"First label example: \", sample_label[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox0WkT12Ctp6"
      },
      "source": [
        "vocab_size += 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5LOMynmEBqj"
      },
      "source": [
        "train_data = configure_dataset(train_data)\n",
        "validation_data = configure_dataset(validation_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtEZ4iuyENcp"
      },
      "source": [
        "model = create_model(vocab_size, num_labels=3)\n",
        "model.compile(optimizer = 'adam', loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics = ['accuracy'])\n",
        "history = model.fit(train_data, validation_data = validation_data, epochs = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-88n6b8_GBRU"
      },
      "source": [
        "loss, acc = model.evaluate(validation_data)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Acc: \", acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YAROSNnAeN7"
      },
      "source": [
        "preprocess_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    max_tokens=vocab_size, standardize = tf_text.case_fold_utf8, split=tokenizer.tokenize, output_mode = 'int', output_sequence_length=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSxEjQjpIyJD"
      },
      "source": [
        "preprocess_layer.set_vocabulary(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLPKHz2CI_rp"
      },
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "    preprocess_layer,\n",
        "    model,\n",
        "    tf.keras.layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "export_model.compile(optimizer = 'adam', loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snzyiGEfJm9v"
      },
      "source": [
        "test_ds = all_labeled_data.take(VALIDATION_SIZE).batch(BATCH_SIZE)\n",
        "test_ds = configure_dataset(test_ds)\n",
        "loss, acc = export_model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Acc: \", acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSQQyMM3Kfju"
      },
      "source": [
        "inputs = [\n",
        "    \"Join'd to th' Ionians with their flowing robes,\",  # Label: 1\n",
        "    \"the allies, and his armour flashed about him so that he seemed to all\",  # Label: 2\n",
        "    \"And with loud clangor of his arms he fell.\",  # Label: 0\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEqAZNv5KjhB"
      },
      "source": [
        "predicted_scores = export_model.predict(inputs)\n",
        "predicted_labels = tf.argmax(predicted_scores, axis=1)\n",
        "\n",
        "for input, label in zip(inputs, predicted_labels):\n",
        "    print(\"Q: \", input)\n",
        "    print(\"L: \", label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxvK84v3LBCe"
      },
      "source": [
        "train_ds = tfds.load('imdb_reviews', split = 'train', batch_size = BATCH_SIZE, shuffle_files = True, as_supervised = True)\n",
        "valid_ds = tfds.load('imdb_reviews', split = 'train', batch_size = BATCH_SIZE, shuffle_files = True, as_supervised = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS7fGmcLNMjN"
      },
      "source": [
        "for review_batch, label_batch in valid_ds.take(1):\n",
        "    for i in range(5):\n",
        "        print(\"Review: \", review_batch[i])\n",
        "        print(\"Label: \", label_batch[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjtdvYklNpSn"
      },
      "source": [
        "vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    max_tokens = VOCAB_SIZE,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = MAX_SEQUENCE_LENGTH,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExpgDWLpOZZe"
      },
      "source": [
        "train_text = train_ds.map(lambda text, label: text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR_31HsGOlDM"
      },
      "source": [
        "vectorize_layer.adapt(train_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SO9sn2NuOriF"
      },
      "source": [
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whDFdP6-PrwD"
      },
      "source": [
        "train_ds = train_ds.map(vectorize_text)\n",
        "valid_ds = valid_ds.map(vectorize_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkYXKcXrQC6i"
      },
      "source": [
        "train_ds = configure_dataset(train_ds)\n",
        "valid_ds = configure_dataset(valid_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN5ugN7nQYjy"
      },
      "source": [
        "model = create_model(VOCAB_SIZE + 1, num_labels = 1)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ElGw15aQlQI"
      },
      "source": [
        "model.compile(optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX-fKVKFQ2Gx"
      },
      "source": [
        "history = model.fit(train_ds, validation_data=valid_ds, epochs=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RHQ2Rx1Q8Z6"
      },
      "source": [
        "loss, acc = model.evaluate(valid_ds)\n",
        "\n",
        "print('Loss: ', loss)\n",
        "print('Acc: ', acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c2CYvLFRHMX"
      },
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "    vectorize_layer,\n",
        "    model,\n",
        "    tf.keras.layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "export_model.compile(optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy(from_logits = True), metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX0wLAr3SY45"
      },
      "source": [
        "inputs = [\n",
        "    \"This is a fantastic movie.\",\n",
        "    \"This is a bad movie.\",\n",
        "    \"This movie was so bad that it was good.\",\n",
        "    \"I will never say yes to watching this movie.\",\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgZUnBPuSb3y"
      },
      "source": [
        "predicted_scores = export_model.predict(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIoCK-WJSgpg"
      },
      "source": [
        "predicted_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOKP4PXbSh6e"
      },
      "source": [
        "for x in predicted_scores:\n",
        "    print(x)\n",
        "    print(x[0])\n",
        "    print(round(x[0]))\n",
        "    print(int(x[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_bfdoVsS_rj"
      },
      "source": [
        "predicted_labels = [round(x[0]) for x in predicted_scores]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBY6JeBTTR0U"
      },
      "source": [
        "for input, label in zip(inputs, predicted_labels):\n",
        "    print(\"Input: \", input)\n",
        "    print(\"Label: \", label)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}